{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"-z5k9m1zp_Uj","colab_type":"code","outputId":"80bc7763-e7fd-4162-95ee-5a91887f73db","executionInfo":{"status":"ok","timestamp":1579501424242,"user_tz":-330,"elapsed":11244,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":350}},"source":["!pip install numpy==1.15\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting numpy==1.15\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/29/f4c845648ed23264e986cdc5fbab5f8eace1be5e62144ef69ccc7189461d/numpy-1.15.0-cp36-cp36m-manylinux1_x86_64.whl (13.9MB)\n","\u001b[K     |████████████████████████████████| 13.9MB 243kB/s \n","\u001b[31mERROR: tensorflow 1.15.0 has requirement numpy<2.0,>=1.16.0, but you'll have numpy 1.15.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: plotnine 0.6.0 has requirement numpy>=1.16.0, but you'll have numpy 1.15.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: astropy 4.0 has requirement numpy>=1.16, but you'll have numpy 1.15.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy\n","  Found existing installation: numpy 1.17.5\n","    Uninstalling numpy-1.17.5:\n","      Successfully uninstalled numpy-1.17.5\n","Successfully installed numpy-1.15.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"ZhejGDWtHOqm","colab_type":"code","outputId":"7dd77dea-5c90-42c7-d7e4-ecfd7284db4c","executionInfo":{"status":"ok","timestamp":1579507534517,"user_tz":-330,"elapsed":28573,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["\n","\n","# Upload the train file from your local drive\n","\n","from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True)\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Dj7VM4bLHYbc","colab_type":"code","outputId":"620d539f-90e2-4b70-b294-0ad2831d182d","executionInfo":{"status":"ok","timestamp":1579507535359,"user_tz":-330,"elapsed":29371,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cd /content/drive/My\\ Drive/Academics/Project/BERT/Bert-Multi-Label-Text-Classification-master/"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Academics/Project/BERT/Bert-Multi-Label-Text-Classification-master\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"o8vMfZdCHbH6","colab_type":"code","colab":{}},"source":["%pycat pybert/test/predictor.py"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ygeV-jmIJQ5d","colab_type":"code","colab":{}},"source":["%pycat  run_bert.py"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ALqQvJn1n4Hm","colab_type":"code","outputId":"9c9a14a5-d412-462a-dfbd-6e4e863c48c1","executionInfo":{"status":"ok","timestamp":1579250832775,"user_tz":-330,"elapsed":1431,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%%writefile run_bert.py\n","import torch\n","import numpy as np\n","import warnings\n","import csv\n","from pathlib import Path\n","from argparse import ArgumentParser\n","from pybert.train.losses import BCEWithLogLoss\n","from pybert.train.trainer import Trainer\n","from torch.utils.data import DataLoader\n","from pybert.io.bert_processor import BertProcessor\n","from pybert.common.tools import init_logger, logger\n","from pybert.common.tools import seed_everything\n","from pybert.configs.basic_config import config\n","from pybert.model.nn.bert_for_multi_label import BertForMultiLable\n","from pybert.preprocessing.preprocessor import EnglishPreProcessor\n","from pybert.callback.modelcheckpoint import ModelCheckpoint\n","from pybert.callback.trainingmonitor import TrainingMonitor\n","from pybert.train.metrics import AUC, AccuracyThresh, MultiLabelReport,Recall,Acc\n","from pytorch_transformers import AdamW, WarmupLinearSchedule\n","from torch.utils.data import RandomSampler, SequentialSampler\n","\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","\n","def run_train(args):\n","    # --------- data\n","    processor = BertProcessor(vocab_path=config['bert_vocab_path'], do_lower_case=args.do_lower_case)\n","    label_list = processor.get_labels()\n","    label2id = {label: i for i, label in enumerate(label_list)}\n","    id2label = {i: label for i, label in enumerate(label_list)}\n","\n","    train_data = processor.get_train(config['data_dir'] / f\"{args.data_name}.train.pkl\")\n","    train_examples = processor.create_examples(lines=train_data,\n","                                               example_type='train',\n","                                               cached_examples_file=config[\n","                                                    'data_dir'] / f\"cached_train_examples_{args.arch}\")\n","    train_features = processor.create_features(examples=train_examples,\n","                                               max_seq_len=args.train_max_seq_len,\n","                                               cached_features_file=config[\n","                                                    'data_dir'] / \"cached_train_features_{}_{}\".format(\n","                                                   args.train_max_seq_len, args.arch\n","                                               ))\n","#    print(train_features[0].input_ids)\n","#    print(train_features[0].segment_ids)\n","#    print(train_features[0].label_id)\n","#    print(train_features[0].input_mask)\n","    train_dataset = processor.create_dataset(train_features, is_sorted=args.sorted)\n","    if args.sorted:\n","        train_sampler = SequentialSampler(train_dataset)\n","    else:\n","        train_sampler = RandomSampler(train_dataset)\n","    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n","\n","    valid_data = processor.get_dev(config['data_dir'] / f\"{args.data_name}.valid.pkl\")\n","    valid_examples = processor.create_examples(lines=valid_data,\n","                                               example_type='valid',\n","                                               cached_examples_file=config[\n","                                                                        'data_dir'] / f\"cached_valid_examples_{args.arch}\")\n","\n","    valid_features = processor.create_features(examples=valid_examples,\n","                                               max_seq_len=args.eval_max_seq_len,\n","                                               cached_features_file=config[\n","                                                                        'data_dir'] / \"cached_valid_features_{}_{}\".format(\n","                                                   args.eval_max_seq_len, args.arch\n","                                               ))\n","    valid_dataset = processor.create_dataset(valid_features)\n","    valid_sampler = SequentialSampler(valid_dataset)\n","    valid_dataloader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=args.eval_batch_size)\n","\n","    # ------- model\n","    logger.info(\"initializing model\")\n","    if args.resume_path:\n","        args.resume_path = Path(args.resume_path)\n","        model = BertForMultiLable.from_pretrained(args.resume_path)\n","        # model.unfreeze(0,11)\n","    else:\n","        model = BertForMultiLable.from_pretrained(config['bert_model_dir'])\n","        # model.unfreeze(0,11)\n","    t_total = int(len(train_dataloader) / args.gradient_accumulation_steps * args.epochs)\n","\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = ['bias', 'LayerNorm.weight']\n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],'weight_decay': args.weight_decay},\n","        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","    ]\n","    warmup_steps = int(t_total * args.warmup_proportion)\n","    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n","    lr_scheduler = WarmupLinearSchedule(optimizer, warmup_steps=warmup_steps, t_total=t_total)\n","\n","    if args.fp16:\n","        try:\n","            from apex import amp\n","        except ImportError:\n","            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n","        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n","\n","    # ---- callbacks\n","    logger.info(\"initializing callbacks\")\n","    train_monitor = TrainingMonitor(file_dir=config['figure_dir'], arch=args.arch)\n","    model_checkpoint = ModelCheckpoint(checkpoint_dir=config['checkpoint_dir'],mode=args.mode,\n","                                       monitor=args.monitor,arch=args.arch,\n","                                       save_best_only=args.save_best)\n","\n","    # **************************** training model ***********************\n","    logger.info(\"***** Running training *****\")\n","    logger.info(\"  Num examples = %d\", len(train_examples))\n","    logger.info(\"  Num Epochs = %d\", args.epochs)\n","    logger.info(\"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n","                args.train_batch_size * args.gradient_accumulation_steps * (\n","                    torch.distributed.get_world_size() if args.local_rank != -1 else 1))\n","    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n","    logger.info(\"  Total optimization steps = %d\", t_total)\n","\n","    trainer = Trainer(n_gpu=args.n_gpu,\n","                      model=model,\n","                      epochs=args.epochs,\n","                      logger=logger,\n","                      criterion=BCEWithLogLoss(reduction='sum'),\n","                      optimizer=optimizer,\n","                      lr_scheduler=lr_scheduler,\n","                      early_stopping=None,\n","                      training_monitor=train_monitor,\n","                      fp16=args.fp16,\n","                      resume_path=args.resume_path,\n","                      grad_clip=args.grad_clip,\n","                      model_checkpoint=model_checkpoint,\n","                      gradient_accumulation_steps=args.gradient_accumulation_steps,\n","                      batch_metrics=[AccuracyThresh(thresh=0.5),Recall(),Acc()],\n","                      epoch_metrics=[#AUC(average='micro', task_type='binary'),\n","                                     #MultiLabelReport(id2label=id2label)\n","                                     AccuracyThresh(thresh=0.5),\n","                                     Recall(),Acc()])\n","    trainer.train(train_data=train_dataloader, valid_data=valid_dataloader, seed=args.seed)\n","\n","\n","def run_test(args,test=False,k=7,med_map='pybert/dataset/med_map.csv'):\n","    from pybert.io.task_data import TaskData\n","    from pybert.test.predictor import Predictor\n","    data = TaskData()\n","    targets, sentences = data.read_data (raw_data_path=config['test_path'],\n","                                        preprocessor=EnglishPreProcessor(),\n","                                        is_train=test)\n","    print(f'-----------------------------------------\\ntargets {targets}\\n---------------------------------------------------')\n","    lines = list(zip(sentences, targets))\n","    processor = BertProcessor(vocab_path=config['bert_vocab_path'], do_lower_case=args.do_lower_case)\n","    label_list = processor.get_labels()\n","    id2label = {i: label for i, label in enumerate(label_list)}\n","\n","    test_data = processor.get_test(lines=lines)\n","    test_examples = processor.create_examples(lines=test_data,\n","                                              example_type='test',\n","                                              cached_examples_file=config[\n","                                                                       'data_dir'] / f\"cached_test_examples_{args.arch}\")\n","    test_features = processor.create_features(examples=test_examples,\n","                                              max_seq_len=args.eval_max_seq_len,\n","                                              cached_features_file=config[\n","                                                                       'data_dir'] / \"cached_test_features_{}_{}\".format(\n","                                                  args.eval_max_seq_len, args.arch\n","                                              ))\n","    test_dataset = processor.create_dataset(test_features)\n","    test_sampler = SequentialSampler(test_dataset)\n","    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=args.train_batch_size)\n","    model = BertForMultiLable.from_pretrained(config['checkpoint_dir'])\n","\n","    # ----------- predicting\n","    logger.info('model predicting....')\n","    predictor = Predictor(model=model,\n","                          logger=logger,\n","                          n_gpu=args.n_gpu, test=test)\n","    if test:\n","        results,targets = predictor.predict(data=test_dataloader)\n","    #print(f'results {results.shape}')\n","    #print(f'targets {targets.shape}')\n","        result =  dict()\n","        metrics = [Recall(),Acc()]\n","        for metric in metrics:\n","                metric.reset()\n","                metric(logits=results, target=targets)\n","                value = metric.value()\n","                if value is not None:\n","                    result[f'valid_{metric.name()}'] = value\n","        return result            \n","    else:\n","        results = predictor.predict(data=test_dataloader)\n","        pred = np.argsort(results)[:,-k:][:,::-1]\n","        with open('pybert/dataset/med_map.csv', mode='r') as infile:\n","          reader = csv.reader(infile)\n","          med_dict = {int(rows[0]):rows[1] for rows in reader}\n","          pred = np.vectorize(med_dict.get)(pred)\n","          return pred\n","\n","\n","def main():\n","    parser = ArgumentParser()\n","    parser.add_argument(\"--arch\", default='bert', type=str)\n","    parser.add_argument(\"--do_data\", action='store_true')\n","    parser.add_argument(\"--do_train\", action='store_true')\n","    parser.add_argument(\"--do_test\", action='store_true')\n","    parser.add_argument(\"--save_best\", action='store_true')\n","    parser.add_argument(\"--do_lower_case\", action='store_true')\n","    parser.add_argument('--data_name', default='kaggle', type=str)\n","    parser.add_argument(\"--epochs\", default=6, type=int)\n","    parser.add_argument(\"--resume_path\", default='', type=str)\n","    parser.add_argument(\"--mode\", default='min', type=str)\n","    parser.add_argument(\"--monitor\", default='valid_loss', type=str)\n","    parser.add_argument(\"--valid_size\", default=0.2, type=float)\n","    parser.add_argument(\"--local_rank\", type=int, default=-1)\n","    parser.add_argument(\"--sorted\", default=1, type=int, help='1 : True  0:False ')\n","    parser.add_argument(\"--n_gpu\", type=str, default='0', help='\"0,1,..\" or \"0\" or \"\" ')\n","    parser.add_argument('--gradient_accumulation_steps', type=int, default=1)\n","    parser.add_argument(\"--train_batch_size\", default=8, type=int)\n","    parser.add_argument('--eval_batch_size', default=8, type=int)\n","    parser.add_argument(\"--train_max_seq_len\", default=256, type=int)\n","    parser.add_argument(\"--eval_max_seq_len\", default=256, type=int)\n","    parser.add_argument('--loss_scale', type=float, default=0)\n","    parser.add_argument(\"--warmup_proportion\", default=0.1, type=int, )\n","    parser.add_argument(\"--weight_decay\", default=0.01, type=float)\n","    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float)\n","    parser.add_argument(\"--grad_clip\", default=1.0, type=float)\n","    parser.add_argument(\"--learning_rate\", default=2e-5, type=float)\n","    parser.add_argument('--seed', type=int, default=42)\n","    parser.add_argument('--fp16', action='store_true')\n","    parser.add_argument('--fp16_opt_level', type=str, default='O1')\n","\n","    args = parser.parse_args()\n","    config['checkpoint_dir'] = config['checkpoint_dir'] / args.arch\n","    config['checkpoint_dir'].mkdir(exist_ok=True)\n","    # Good practice: save your training arguments together with the trained model\n","    torch.save(args, config['checkpoint_dir'] / 'training_args.bin')\n","    seed_everything(args.seed)\n","    init_logger(log_file=config['log_dir'] / f\"{args.arch}.log\")\n","\n","    logger.info(\"Training/evaluation parameters %s\", args)\n","\n","    if args.do_data:\n","        from pybert.io.task_data import TaskData\n","        data = TaskData()\n","        targets, sentences = data.read_data(raw_data_path=config['raw_data_path'],\n","                                            preprocessor=EnglishPreProcessor(),\n","                                            is_train=True)\n","        data.train_val_split(X=sentences, y=targets, shuffle=True, stratify=False,\n","                             valid_size=args.valid_size, data_dir=config['data_dir'],\n","                             data_name=args.data_name)\n","    if args.do_train:\n","        run_train(args)\n","\n","    if args.do_test:\n","      test = False\n","      pred = run_test(args,test=test)\n","      if(test):\n","        print(f'{pred}\\n\\n')\n","      else:\n","        print(\"\\n\")\n","        for pat in pred:\n","          for i,med in enumerate(pat):\n","            print(f'{i+1} {med}\\n')\n","          print(\"\\n\\n\")\n","\n","\n","if __name__ == '__main__':\n","    main()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Overwriting run_bert.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Yg6UZ0fwTmVO","colab_type":"code","colab":{}},"source":["%pycat  pybert/model/nn/bert_for_multi_label.py"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_7Q3z0yhUPal","colab_type":"code","outputId":"6ebc71d0-8a9c-4e54-fb9d-b71c89467cf4","executionInfo":{"status":"ok","timestamp":1579188995857,"user_tz":-330,"elapsed":2110,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%%writefile  pybert/model/nn/bert_for_multi_label.py\n","\n","import torch.nn as nn\n","from pytorch_transformers.modeling_bert import BertPreTrainedModel, BertModel\n","\n","\n","class GRUNet(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, embedding_dim = None,num_emd = None, drop_prob=0.2):\n","        super(GRUNet, self).__init__()\n","        self.hidden_dim = hidden_dim\n","        self.n_layers = n_layers\n","        \n","#        self.embedding = nn.Embedding(embedding_dim=embedding_dim, num_embeddings=num_emd, padding_idx=0)\n","        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","#        self.softmax = nn.Softmax(dim=2)\n","        \n","    def forward(self, x, h=None):\n","        # out = self.embedding(x)\n","#         print(f'before sum {out.shape}')\n","#         print(out, length)\n","        # out = out.sum(dim=-2)\n","#         print(f'before packing {out.shape}')\n","        # out = torch.nn.utils.rnn.pack_padded_sequence(out, lengths=length, batch_first=True, enforce_sorted=True)\n","#         print(f'after packing {out.data.shape}')\n","        # print(f'x.shape  {x.shape}\\n\\n')\n","        if h is not None:   \n","          out, h = self.gru(x, h)\n","        else:\n","          out, h = self.gru(x)  \n","        # out, out_lengths = torch.nn.utils.rnn.pad_packed_sequence(packed_output,batch_first=True)\n","#         print(f'after unpacking {out.shape} {out_lengths.shape}')\n","        # print(f'out.shape  {out.shape}\\n\\n')\n","        out = self.fc(out[:,-1,:])\n","\n","        # out=self.softmax(out)\n","        return out,h\n","    \n","    def init_hidden(self, batch_size):\n","        weight = next(self.parameters()).data\n","        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n","        return hidden\n","\n","class BertForMultiLable(BertPreTrainedModel):\n","    def __init__(self, config):\n","\n","        super(BertForMultiLable, self).__init__(config)\n","        self.bert = BertModel(config)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        # self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n","        self.classifier = GRUNet(1, hidden_dim = 356, output_dim = config.num_labels, n_layers = 3 ) \n","        self.init_weights()\n","\n","    def forward(self, input_ids, token_type_ids=None, attention_mask=None,head_mask=None, hidden_state = None):\n","        outputs = self.bert(input_ids, token_type_ids=token_type_ids,attention_mask=attention_mask, head_mask=head_mask)\n","        pooled_output = outputs[1]\n","        # print(f'pooled_output.shape {pooled_output.shape} \\n\\n')\n","        pooled_output = pooled_output.unsqueeze(2)\n","        # pooled_output = self.dropout(pooled_output)\n","        # print(f'pooled_output.shape {pooled_output.shape} \\n\\n')\n","\n","        if hidden_state is None:\n","          logits, h = self.classifier(pooled_output)\n","        else:\n","          logits, h = self.classifier(pooled_output, hidden_state)  \n","        # logits = self.classifier(pooled_output)  \n","        # print(f'logits.shape  {logits.shape}   h.shape  {h.shape}\\n\\n')\n","        return logits, h\n","        # return logits   \n","\n","    def unfreeze(self,start_layer,end_layer):\n","        def children(m):\n","            return m if isinstance(m, (list, tuple)) else list(m.children())\n","        def set_trainable_attr(m, b):\n","            m.trainable = b\n","            for p in m.parameters():\n","                p.requires_grad = b\n","        def apply_leaf(m, f):\n","            c = children(m)\n","            if isinstance(m, nn.Module):\n","                f(m)\n","            if len(c) > 0:\n","                for l in c:\n","                    apply_leaf(l, f)\n","        def set_trainable(l, b):\n","            apply_leaf(l, lambda m: set_trainable_attr(m, b))\n","\n","        # You can unfreeze the last layer of bert by calling set_trainable(model.bert.encoder.layer[23], True)\n","        set_trainable(self.bert, False)\n","        for i in range(start_layer, end_layer+1):\n","            set_trainable(self.bert.encoder.layer[i], True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Overwriting pybert/model/nn/bert_for_multi_label.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fqUi7m4dUPVO","colab_type":"code","colab":{}},"source":["%pycat pybert/test/predictor.py"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dybf-DyqZ12z","colab_type":"code","outputId":"8373302e-ad1d-4994-9b51-39b455f54dfb","executionInfo":{"status":"ok","timestamp":1579240696771,"user_tz":-330,"elapsed":681,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%%writefile pybert/test/predictor.py\n","#encoding:utf-8\n","import torch\n","import numpy as np\n","from ..common.tools import model_device\n","from ..callback.progressbar import ProgressBar\n","\n","class Predictor(object):\n","    def __init__(self,\n","                 model,\n","                 logger,\n","                 n_gpu,\n","                 test=False):\n","        self.model = model\n","        self.logger = logger\n","        self.model, self.device = model_device(n_gpu= n_gpu, model=self.model)\n","        self.test = test\n","\n","    def predict(self,data):\n","        pbar = ProgressBar(n_total=len(data))\n","        if self.test: \n","            all_logits = []\n","        else:\n","            all_logits = None\n","        self.model.eval()\n","        targets = []\n","        with torch.no_grad():\n","            if self.test:\n","                for step, batch in enumerate(data):\n","                    batch = tuple(t.to(self.device) for t in batch)\n","                    input_ids, input_mask, segment_ids, label_ids = batch\n","                    # logits = self.model(input_ids, segment_ids, input_mask)\n","                    logits, h = self.model(input_ids, segment_ids, input_mask, hidden_state = None)\n","                    all_logits.append(logits.cpu().detach())\n","                    targets.append(label_ids.cpu().detach())\n","                #print(f'targets {targets}')                            \n","                    pbar.batch_step(step=step,info = {},bar_type='Evaluating')\n","                #print(logits)  \n","                #targets.append(label_ids.cpu().detach()) \n","#                if all_logits is None:\n","#                    all_logits = logits.detach().cpu().numpy()\n","#                else:\n","#                    all_logits = np.concatenate([all_logits,logits.detach().cpu().numpy()],axis = 0)\n","                    pbar.batch_step(step=step,info = {},bar_type='Testing')\n","                all_logits = torch.cat(all_logits, dim = 0).cpu().detach()\n","                targets = torch.cat(targets, dim = 0).cpu().detach()\n","\n","                if 'cuda' in str(self.device):\n","                    torch.cuda.empty_cache()\n","                return all_logits,targets\n","\n","            else:\n","                for step, batch in enumerate(data):\n","                       batch = tuple(t.to(self.device) for t in batch)\n","                       input_ids, input_mask, segment_ids, label_ids = batch\n","                       logits, h = self.model(input_ids, segment_ids, input_mask, hidden_state = None)\n","                      #  logits = self.model(input_ids, segment_ids, input_mask)\n","                       logits = logits.sigmoid()\n","                      #  print(f'logits.shape {logits.shape}')\n","                       if all_logits is None:\n","                           all_logits = logits.detach().cpu().numpy()\n","                       else:\n","                           all_logits = np.concatenate([all_logits,logits.detach().cpu().numpy()],axis = 0)\n","                       pbar.batch_step(step=step,info = {},bar_type='Testing')\n","                if 'cuda' in str(self.device):\n","                    torch.cuda.empty_cache()\n","                return all_logits\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Overwriting pybert/test/predictor.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_Wg7i2MhZ-Jb","colab_type":"code","colab":{}},"source":["%pycat pybert/train/metrics.py\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QkNckL8SIoAR","colab_type":"code","outputId":"8aa127cb-73bb-456b-d1ef-20fb971031c9","executionInfo":{"status":"ok","timestamp":1579015694102,"user_tz":-330,"elapsed":2511,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%%writefile pybert/train/metrics.py\n","r\"\"\"Functional interface\"\"\"\n","import torch\n","import csv\n","from tqdm import tqdm\n","import numpy as np\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import f1_score, classification_report\n","\n","__call__ = ['Accuracy','AUC','F1Score','EntityScore','ClassReport','MultiLabelReport','AccuracyThresh']\n","\n","class Metric:\n","    def __init__(self):\n","        pass\n","\n","    def __call__(self, outputs, target):\n","        raise NotImplementedError\n","\n","    def reset(self):\n","        raise NotImplementedError\n","\n","    def value(self):\n","        raise NotImplementedError\n","\n","    def name(self):\n","        raise NotImplementedError\n","\n","class Accuracy(Metric):\n","    '''\n","    计算准确度\n","    可以使用topK参数设定计算K准确度\n","    Examples:\n","        >>> metric = Accuracy(**)\n","        >>> for epoch in range(epochs):\n","        >>>     metric.reset()\n","        >>>     for batch in batchs:\n","        >>>         logits = model()\n","        >>>         metric(logits,target)\n","        >>>         print(metric.name(),metric.value())\n","    '''\n","    def __init__(self,topK):\n","        super(Accuracy,self).__init__()\n","        self.topK = topK\n","        self.reset()\n","\n","    def __call__(self, logits, target):\n","        _, pred = logits.topk(self.topK, 1, True, True)\n","        pred = pred.t()\n","        correct = pred.eq(target.view(1, -1).expand_as(pred))\n","        self.correct_k = correct[:self.topK].view(-1).float().sum(0)\n","        self.total = target.size(0)\n","\n","    def reset(self):\n","        self.correct_k = 0\n","        self.total = 0\n","\n","    def value(self):\n","        return float(self.correct_k)  / self.total\n","\n","    def name(self):\n","        return 'accuracy'\n","\n","\n","class AccuracyThresh(Metric):\n","    '''\n","    计算准确度\n","    可以使用topK参数设定计算K准确度\n","    Example:\n","        >>> metric = AccuracyThresh(**)\n","        >>> for epoch in range(epochs):\n","        >>>     metric.reset()\n","        >>>     for batch in batchs:\n","        >>>         logits = model()\n","        >>>         metric(logits,target)\n","        >>>         print(metric.name(),metric.value())\n","    '''\n","    def __init__(self,thresh = 0.5):\n","        super(AccuracyThresh,self).__init__()\n","        self.thresh = thresh\n","        self.reset()\n","\n","    def __call__(self, logits, target):\n","#        print(f'xxxxxxxxxxxxxxxxx\\nlogits.shape{logits.shape} target.shape{target.shape}\\nxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx')\n","        self.y_pred = logits.sigmoid()\n","        self.y_true = target\n","\n","    def reset(self):\n","        self.correct_k = 0\n","        self.total = 0\n","\n","    def value(self):\n","        data_size = self.y_pred.size(0)\n","#        print(f'xxxxxxxxxxxxxxxxx\\ndata.size{data_size}\\nxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx')  \n","        acc = np.mean(((self.y_pred>self.thresh)==self.y_true.byte()).float().cpu().numpy(), axis=1).sum()\n","        return acc / data_size\n","\n","    def name(self):\n","        return 'accuracy'\n","\n","\n","class AUC(Metric):\n","    '''\n","    AUC score\n","    micro:\n","            Calculate metrics globally by considering each element of the label\n","            indicator matrix as a label.\n","    macro:\n","            Calculate metrics for each label, and find their unweighted\n","            mean.  This does not take label imbalance into account.\n","    weighted:\n","            Calculate metrics for each label, and find their average, weighted\n","            by support (the number of true instances for each label).\n","    samples:\n","            Calculate metrics for each instance, and find their average.\n","    Example:\n","        >>> metric = AUC(**)\n","        >>> for epoch in range(epochs):\n","        >>>     metric.reset()\n","        >>>     for batch in batchs:\n","        >>>         logits = model()\n","        >>>         metric(logits,target)\n","        >>>         print(metric.name(),metric.value())\n","    '''\n","\n","    def __init__(self,task_type = 'binary',average = 'binary'):\n","        super(AUC, self).__init__()\n","\n","        assert task_type in ['binary','multiclass']\n","        assert average in ['binary','micro', 'macro', 'samples', 'weighted']\n","\n","        self.task_type = task_type\n","        self.average = average\n","\n","    def __call__(self,logits,target):\n","        '''\n","        计算整个结果\n","        '''\n","        if self.task_type == 'binary':\n","            self.y_prob = logits.sigmoid().data.cpu().numpy()\n","        else:\n","            self.y_prob = logits.softmax(-1).data.cpu().detach().numpy()\n","        self.y_true = target.cpu().numpy()\n","\n","    def reset(self):\n","        self.y_prob = 0\n","        self.y_true = 0\n","\n","    def value(self):\n","        '''\n","        计算指标得分\n","        '''\n","        auc = roc_auc_score(y_score=self.y_prob, y_true=self.y_true, average=self.average)\n","        return auc\n","\n","    def name(self):\n","        return 'auc'\n","\n","class F1Score(Metric):\n","    '''\n","    F1 Score\n","    binary:\n","            Only report results for the class specified by ``pos_label``.\n","            This is applicable only if targets (``y_{true,pred}``) are binary.\n","    micro:\n","            Calculate metrics globally by considering each element of the label\n","            indicator matrix as a label.\n","    macro:\n","            Calculate metrics for each label, and find their unweighted\n","            mean.  This does not take label imbalance into account.\n","    weighted:\n","            Calculate metrics for each label, and find their average, weighted\n","            by support (the number of true instances for each label).\n","    samples:\n","            Calculate metrics for each instance, and find their average.\n","    Example:\n","        >>> metric = F1Score(**)\n","        >>> for epoch in range(epochs):\n","        >>>     metric.reset()\n","        >>>     for batch in batchs:\n","        >>>         logits = model()\n","        >>>         metric(logits,target)\n","        >>>         print(metric.name(),metric.value())\n","    '''\n","    def __init__(self,thresh = 0.5, normalizate = True,task_type = 'binary',average = 'binary',search_thresh = False):\n","        super(F1Score).__init__()\n","        assert task_type in ['binary','multiclass']\n","        assert average in ['binary','micro', 'macro', 'samples', 'weighted']\n","\n","        self.thresh = thresh\n","        self.task_type = task_type\n","        self.normalizate  = normalizate\n","        self.search_thresh = search_thresh\n","        self.average = average\n","\n","    def thresh_search(self,y_prob):\n","        '''\n","        对于f1评分的指标，一般我们需要对阈值进行调整，一般不会使用默认的0.5值，因此\n","        这里我们队Thresh进行优化\n","        :return:\n","        '''\n","        best_threshold = 0\n","        best_score = 0\n","        for threshold in tqdm([i * 0.01 for i in range(100)], disable=True):\n","            self.y_pred = y_prob > threshold\n","            score = self.value()\n","            if score > best_score:\n","                best_threshold = threshold\n","                best_score = score\n","        return best_threshold,best_score\n","\n","    def __call__(self,logits,target):\n","        '''\n","        计算整个结果\n","        :return:\n","        '''\n","        self.y_true = target.cpu().numpy()\n","        if self.normalizate and self.task_type == 'binary':\n","            y_prob = logits.sigmoid().data.cpu().numpy()\n","        elif self.normalizate and self.task_type == 'multiclass':\n","            y_prob = logits.softmax(-1).data.cpu().detach().numpy()\n","        else:\n","            y_prob = logits.cpu().detach().numpy()\n","\n","        if self.task_type == 'binary':\n","            if self.thresh and self.search_thresh == False:\n","                self.y_pred = (y_prob > self.thresh ).astype(int)\n","                self.value()\n","            else:\n","                thresh,f1 = self.thresh_search(y_prob = y_prob)\n","                print(f\"Best thresh: {thresh:.4f} - F1 Score: {f1:.4f}\")\n","\n","        if self.task_type == 'multiclass':\n","            self.y_pred = np.argmax(y_prob, 1)\n","\n","    def reset(self):\n","        self.y_pred = 0\n","        self.y_true = 0\n","\n","    def value(self):\n","        '''\n","         计算指标得分\n","         '''\n","        f1 = f1_score(y_true=self.y_true, y_pred=self.y_pred, average=self.average)\n","        return f1\n","\n","    def name(self):\n","        return 'f1'\n","\n","class ClassReport(Metric):\n","    '''\n","    class report\n","    '''\n","    def __init__(self,target_names = None):\n","        super(ClassReport).__init__()\n","        self.target_names = target_names\n","\n","    def reset(self):\n","        self.y_pred = 0\n","        self.y_true = 0\n","\n","    def value(self):\n","        '''\n","        计算指标得分\n","        '''\n","        score = classification_report(y_true = self.y_true,\n","                                      y_pred = self.y_pred,\n","                                      target_names=self.target_names)\n","        print(f\"\\n\\n classification report: {score}\")\n","\n","    def __call__(self,logits,target):\n","        _, y_pred = torch.max(logits.data, 1)\n","        self.y_pred = y_pred.cpu().numpy()\n","        self.y_true = target.cpu().numpy()\n","\n","    def name(self):\n","        return \"class_report\"\n","\n","class MultiLabelReport(Metric):\n","    '''\n","    multi label report\n","    '''\n","    def __init__(self,id2label = None):\n","        super(MultiLabelReport).__init__()\n","        self.id2label = id2label\n","\n","    def reset(self):\n","        self.y_prob = 0\n","        self.y_true = 0\n","\n","    def __call__(self,logits,target):\n","\n","        self.y_prob = logits.sigmoid().data.cpu().detach().numpy()\n","        self.y_true = target.cpu().numpy()\n","\n","    def value(self):\n","        '''\n","        计算指标得分\n","        '''\n","        for i, label in self.id2label.items():\n","            auc = roc_auc_score(y_score=self.y_prob[:, i], y_true=self.y_true[:, i])\n","            print(f\"label:{label} - auc: {auc:.4f}\")\n","\n","    def name(self):\n","        return \"multilabel_report\"\n","\n","\n","class Recall(Metric):\n","    '''\n","    Recall\n","    '''\n","    def __init__(self,k=7):\n","        super(Recall).__init__()\n","        self.k=k \n","        \n","    def reset(self):\n","        self.pred_label = []\n","        self.true_label = []\n","\n","    def __call__(self,logits,target):\n","\n","        y_prob = logits.sigmoid().data.cpu().detach().numpy()\n","        y_true = target.cpu().numpy()\n","        \n","        self.true_label = [np.where(x==1)[0] for x in y_true]\n","        for p,q in zip(self.true_label,y_prob):\n","            self.pred_label.append(np.argsort(q)[-self.k:][::-1])\n","                \n","          \n","        \n","    def value(self):\n","        '''\n","        计算指标得分\n","        '''\n","        recall = []\n","        with open('pybert/dataset/med_map.csv', mode='r') as infile:\n","          reader = csv.reader(infile)\n","          med_dict = {int(rows[0]):rows[1] for rows in reader}\n","          \n","        \n","\n","        #print(f'self.true_label {self.true_label}\\n\\n\\n')\n","        #print(f'self.pred_label {self.pred_label}\\n\\n\\n') \n","        for p,q in zip(self.true_label,self.pred_label):\n","\n","            TP = (np.intersect1d(q,p)).size\n","#              print(f'p.size  {p.size} \\n q q \\n pred {pred} \\n p {p} \\n')\n","            try :    \n","                rec = TP/p.size\n","            except ZeroDivisionError as z:\n","                print(f'p.size {p.size}\\n')\n","                continue\n","\n","            true = np.vectorize(med_dict.get)(p)\n","            pred = np.vectorize(med_dict.get)(q)\n","\n","            # print(f'true label {p}\\n')\n","            # for i,med in enumerate(true):\n","            #   print(f'{i} {med}\\n')\n","\n","            # print(f'\\npred label {q}\\n')\n","            # for i,med in enumerate(pred):\n","            #   print(f'{i} {med}\\n')\n","\n","            recall.append(rec)\n","            # print(f'\\navg recall {np.mean(recall)}\\n\\n\\n\\n\\n\\n\\n\\n')\n","        return np.mean(recall)\n","\n","    def name(self):\n","        return \"Recall\"\n","\n","\n","\n","class Acc(Metric):\n","    '''\n","    Accuracy\n","    '''\n","    def __init__(self,k=7):\n","        super(Acc).__init__()\n","        \n","    def reset(self):\n","        self.pred_label = []\n","        self.true_label = []\n","\n","    def __call__(self,logits,target):\n","\n","        y_prob = logits.sigmoid().data.cpu().detach().numpy()\n","        y_true = target.cpu().numpy()\n","        \n","        self.true_label = [np.where(x==1)[0] for x in y_true]\n","        for p,q in zip(self.true_label,y_prob):\n","            self.pred_label.append(np.argsort(q)[-len(p):][::-1])\n","                \n","          \n","        \n","    def value(self):\n","        '''\n","        计算指标得分\n","        '''\n","        accuracy = []\n","        # pred = np.argsort(results)[:,-k:]\n","        with open('pybert/dataset/med_map.csv', mode='r') as infile:\n","          reader = csv.reader(infile)\n","          med_dict = {int(rows[0]):rows[1] for rows in reader}\n","          \n","        \n","        for p,q in zip(self.true_label,self.pred_label):\n","\n","            TP = (np.intersect1d(q,p)).size\n","#                 print(f'p.size  {p.size} \\n q q \\n pred {pred} \\n p {p} \\n')\n","#            acc = TP/p.size\n","            try :    \n","                acc = TP/p.size\n","            except ZeroDivisionError as z:\n","                continue\n","\n","            true = np.vectorize(med_dict.get)(p)\n","            pred = np.vectorize(med_dict.get)(q)\n","\n","            # print(f'true label {p}\\n')\n","            # for i,med in enumerate(true):\n","            #   print(f'{i} {med}\\n')\n","\n","            # print(f'\\npred label {q}\\n')\n","            # for i,med in enumerate(pred):\n","              # print(f'{i} {med}\\n')\n","\n","            accuracy.append(acc)\n","            # print(f'\\navg accuracy {np.mean(accuracy)}\\n\\n\\n\\n\\n\\n\\n\\n')\n","                 \n","        return np.mean(accuracy)        \n","\n","\n","    def name(self):\n","        return \"Acc\""],"execution_count":0,"outputs":[{"output_type":"stream","text":["Overwriting pybert/train/metrics.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pwdUw_X9KlkB","colab_type":"code","colab":{}},"source":["%pycat pybert/io/task_data.py"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4yIaaw9M4E1C","colab_type":"code","outputId":"e4c58d5f-44e4-49ed-b675-216b7726db59","executionInfo":{"status":"ok","timestamp":1577375997549,"user_tz":-330,"elapsed":1904,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%%writefile pybert/io/task_data.py\n","import random\n","import pandas as pd\n","from tqdm import tqdm\n","from ..common.tools import save_pickle\n","from ..common.tools import logger\n","from ..callback.progressbar import ProgressBar\n","\n","class TaskData(object):\n","    def __init__(self):\n","        pass\n","    def train_val_split(self,X, y,valid_size,stratify=False,shuffle=True,save = True,\n","                        seed = None,data_name = None,data_dir = None):\n","        pbar = ProgressBar(n_total=len(X))\n","        logger.info('split raw data into train and valid')\n","        if stratify:\n","            num_classes = len(list(set(y)))\n","            train, valid = [], []\n","            bucket = [[] for _ in range(num_classes)]\n","            for step,(data_x, data_y) in enumerate(zip(X, y)):\n","                bucket[int(data_y)].append((data_x, data_y))\n","                pbar.batch_step(step=step,info = {},bar_type='bucket')\n","            del X, y\n","            for bt in tqdm(bucket, desc='split'):\n","                N = len(bt)\n","                if N == 0:\n","                    continue\n","                test_size = int(N * valid_size)\n","                if shuffle:\n","                    random.seed(seed)\n","                    random.shuffle(bt)\n","                valid.extend(bt[:test_size])\n","                train.extend(bt[test_size:])\n","            if shuffle:\n","                random.seed(seed)\n","                random.shuffle(train)\n","        else:\n","            data = []\n","            for step,(data_x, data_y) in enumerate(zip(X, y)):\n","                data.append((data_x, data_y))\n","                pbar.batch_step(step=step, info={}, bar_type='merge')\n","            del X, y\n","            N = len(data)\n","            test_size = int(N * valid_size)\n","            if shuffle:\n","                random.seed(seed)\n","                random.shuffle(data)\n","            valid = data[:test_size]\n","            train = data[test_size:]\n","            # 混洗train数据集\n","            if shuffle:\n","                random.seed(seed)\n","                random.shuffle(train)\n","        if save:\n","            train_path = data_dir / f\"{data_name}.train.pkl\"\n","            valid_path = data_dir / f\"{data_name}.valid.pkl\"\n","            save_pickle(data=train,file_path=train_path)\n","            save_pickle(data = valid,file_path=valid_path)\n","        return train, valid\n","\n","    def read_data(self,raw_data_path,preprocessor = None,is_train=True):\n","        '''\n","        :param raw_data_path:\n","        :param skip_header:\n","        :param preprocessor:\n","        :return:\n","        '''\n","        targets, sentences = [], []\n","        data = pd.read_hdf(raw_data_path)               \n","        for row in data.values:\n","            if is_train:\n","                target = row[-1]\n","                sentence = str(row[-3])\n","            else:\n","                sentence = str(row[-1])\n","                target = [-1,-1,-1,-1,-1,-1]\n","            print('\\n\\n',sentence)    \n","            if preprocessor:\n","                sentence = preprocessor(sentence)\n","            if sentence:\n","                targets.append(target)\n","                sentences.append(sentence)\n","        return targets,sentences"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Overwriting pybert/io/task_data.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9-i3zRvh4ZJB","colab_type":"code","outputId":"7aa9b74f-1fb4-4fa2-a7ee-8efa49427f01","executionInfo":{"status":"error","timestamp":1577342983652,"user_tz":-330,"elapsed":2101,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":364}},"source":["import pandas as pd\n","df = pd.read_hdf('pybert/dataset/test.csv')"],"execution_count":0,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-b91ed6c03a7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_hdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pybert/dataset/test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mread_hdf\u001b[0;34m(path_or_buf, key, mode, **kwargs)\u001b[0m\n\u001b[1;32m    405\u001b[0m                     )\n\u001b[1;32m    406\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcandidate_only_group\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_v_pathname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauto_close\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauto_close\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;31m# if there is an error, close the store\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, key, where, start, stop, columns, iterator, chunksize, auto_close, **kwargs)\u001b[0m\n\u001b[1;32m    780\u001b[0m         )\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mselect_as_coordinates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self, coordinates)\u001b[0m\n\u001b[1;32m   1637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m         \u001b[0;31m# directly return the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1639\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1640\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mfunc\u001b[0;34m(_start, _stop, _where)\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;31m# function to call on iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_where\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_where\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0;31m# create the iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, start, stop, **kwargs)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0mblk_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"block{idx}_items\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             values = self.read_array(\n\u001b[0;32m-> 3206\u001b[0;31m                 \u001b[0;34m\"block{idx}_values\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3207\u001b[0m             )\n\u001b[1;32m   3208\u001b[0m             \u001b[0mblk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk_items\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(self, key, start, stop)\u001b[0m\n\u001b[1;32m   2735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2736\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVLArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2737\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2739\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"value_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tables/vlarray.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m             start, stop, step = self._process_range(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tables/vlarray.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, start, stop, step)\u001b[0m\n\u001b[1;32m    819\u001b[0m             \u001b[0mlistarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m             \u001b[0mlistarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0matom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mtables/hdf5extension.pyx\u001b[0m in \u001b[0;36mtables.hdf5extension.VLArray._read_array\u001b[0;34m()\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: cannot set WRITEABLE flag to True of this array"]}]},{"cell_type":"code","metadata":{"id":"a7q1bV3B99Uk","colab_type":"code","colab":{}},"source":["%pycat pybert/io/bert_processor"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nPz2wOisAstk","colab_type":"code","outputId":"73f2b8c6-5ac8-4387-ad7e-932ba0e78f97","executionInfo":{"status":"ok","timestamp":1577343765387,"user_tz":-330,"elapsed":1110,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%%writefile pybert/io/bert_processor\n","import csv\n","import torch\n","import numpy as np\n","from ..common.tools import load_pickle\n","from ..common.tools import logger\n","from ..callback.progressbar import ProgressBar\n","from torch.utils.data import TensorDataset\n","from pytorch_transformers import BertTokenizer\n","\n","class InputExample(object):\n","    def __init__(self, guid, text_a, text_b=None, label=None):\n","        \"\"\"Constructs a InputExample.\n","        Args:\n","            guid: Unique id for the example.\n","            text_a: string. The untokenized text of the first sequence. For single\n","            sequence tasks, only this sequence must be specified.\n","            text_b: (Optional) string. The untokenized text of the second sequence.\n","            Only must be specified for sequence pair tasks.\n","            label: (Optional) string. The label of the example. This should be\n","            specified for train and dev examples, but not for test examples.\n","        \"\"\"\n","        self.guid   = guid\n","        self.text_a = text_a\n","        self.text_b = text_b\n","        self.label  = label\n","\n","class InputFeature(object):\n","    '''\n","    A single set of features of data.\n","    '''\n","    def __init__(self,input_ids,input_mask,segment_ids,label_id,input_len):\n","        self.input_ids   = input_ids\n","        self.input_mask  = input_mask\n","        self.segment_ids = segment_ids\n","        self.label_id    = label_id\n","        self.input_len = input_len\n","\n","class BertProcessor(object):\n","    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n","\n","    def __init__(self,vocab_path,do_lower_case):\n","        self.tokenizer = BertTokenizer(vocab_path,do_lower_case)\n","\n","    def get_train(self, data_file):\n","        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n","        return self.read_data(data_file)\n","\n","    def get_dev(self, data_file):\n","        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n","        return self.read_data(data_file)\n","\n","    def get_test(self,lines):\n","        return lines\n","\n","    def get_labels(self):\n","        \"\"\"Gets the list of labels for this data set.\"\"\"\n","        return [\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\n","\n","    @classmethod\n","    def read_data(cls, input_file,quotechar = None):\n","        \"\"\"Reads a tab separated value file.\"\"\"\n","        if 'pkl' in str(input_file):\n","            lines = load_pickle(input_file)\n","        else:\n","            lines = input_file\n","        return lines\n","\n","    def truncate_seq_pair(self,tokens_a,tokens_b,max_length):\n","        # This is a simple heuristic which will always truncate the longer sequence\n","        # one token at a time. This makes more sense than truncating an equal percent\n","        # of tokens from each, since if one sequence is very short then each token\n","        # that's truncated likely contains more information than a longer sequence.\n","        while True:\n","            total_length = len(tokens_a) + len(tokens_b)\n","            if total_length <= max_length:\n","                break\n","            if len(tokens_a) > len(tokens_b):\n","                tokens_a.pop()\n","            else:\n","                tokens_b.pop()\n","\n","    def create_examples(self,lines,example_type,cached_examples_file):\n","        '''\n","        Creates examples for data\n","        '''\n","        pbar = ProgressBar(n_total = len(lines))\n","        if cached_examples_file.exists():\n","            logger.info(\"Loading examples from cached file %s\", cached_examples_file)\n","            examples = torch.load(cached_examples_file)\n","        else:\n","            examples = []\n","            for i,line in enumerate(lines):\n","                guid = '%s-%d'%(example_type,i)\n","                text_a = line[0]\n","                label = line[1]\n","                print('label',label)\n","                if isinstance(label,str):\n","                    label = [np.float(x) for x in label.split(\",\")]\n","                else:\n","                    label = [np.float(x) for x in label]\n","                text_b = None\n","                example = InputExample(guid = guid,text_a = text_a,text_b=text_b,label= label)\n","                examples.append(example)\n","                pbar.batch_step(step=i,info={},bar_type='create examples')\n","            logger.info(\"Saving examples into cached file %s\", cached_examples_file)\n","            torch.save(examples, cached_examples_file)\n","        return examples\n","\n","    def create_features(self,examples,max_seq_len,cached_features_file):\n","        '''\n","        # The convention in BERT is:\n","        # (a) For sequence pairs:\n","        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n","        #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n","        # (b) For single sequences:\n","        #  tokens:   [CLS] the dog is hairy . [SEP]\n","        #  type_ids:   0   0   0   0  0     0   0\n","        '''\n","        pbar = ProgressBar(n_total=len(examples))\n","        if cached_features_file.exists():\n","            logger.info(\"Loading features from cached file %s\", cached_features_file)\n","            features = torch.load(cached_features_file)\n","        else:\n","            features = []\n","            for ex_id,example in enumerate(examples):\n","                tokens_a = self.tokenizer.tokenize(example.text_a)\n","                tokens_b = None\n","                label_id = example.label\n","\n","                if example.text_b:\n","                    tokens_b = self.tokenizer.tokenize(example.text_b)\n","                    # Modifies `tokens_a` and `tokens_b` in place so that the total\n","                    # length is less than the specified length.\n","                    # Account for [CLS], [SEP], [SEP] with \"- 3\"\n","                    self.truncate_seq_pair(tokens_a,tokens_b,max_length = max_seq_len - 3)\n","                else:\n","                    # Account for [CLS] and [SEP] with '-2'\n","                    if len(tokens_a) > max_seq_len - 2:\n","                        tokens_a = tokens_a[:max_seq_len - 2]\n","                tokens = ['[CLS]'] + tokens_a + ['[SEP]']\n","                segment_ids = [0] * len(tokens)\n","                if tokens_b:\n","                    tokens += tokens_b + ['[SEP]']\n","                    segment_ids += [1] * (len(tokens_b) + 1)\n","\n","                input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n","                input_mask = [1] * len(input_ids)\n","                padding = [0] * (max_seq_len - len(input_ids))\n","                input_len = len(input_ids)\n","\n","                input_ids   += padding\n","                input_mask  += padding\n","                segment_ids += padding\n","\n","                assert len(input_ids) == max_seq_len\n","                assert len(input_mask) == max_seq_len\n","                assert len(segment_ids) == max_seq_len\n","\n","                if ex_id < 2:\n","                    logger.info(\"*** Example ***\")\n","                    logger.info(f\"guid: {example.guid}\" % ())\n","                    logger.info(f\"tokens: {' '.join([str(x) for x in tokens])}\")\n","                    logger.info(f\"input_ids: {' '.join([str(x) for x in input_ids])}\")\n","                    logger.info(f\"input_mask: {' '.join([str(x) for x in input_mask])}\")\n","                    logger.info(f\"segment_ids: {' '.join([str(x) for x in segment_ids])}\")\n","\n","                feature = InputFeature(input_ids = input_ids,\n","                                       input_mask = input_mask,\n","                                       segment_ids = segment_ids,\n","                                       label_id = label_id,\n","                                       input_len = input_len)\n","                features.append(feature)\n","                pbar.batch_step(step=ex_id, info={}, bar_type='create features')\n","            logger.info(\"Saving features into cached file %s\", cached_features_file)\n","            torch.save(features, cached_features_file)\n","        return features\n","\n","    def create_dataset(self,features,is_sorted = False):\n","        # Convert to Tensors and build dataset\n","        if is_sorted:\n","            logger.info(\"sorted data by th length of input\")\n","            features = sorted(features,key=lambda x:x.input_len,reverse=True)\n","        all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n","        all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n","        all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n","#        for i in features:\n","#            print(i.label_id)\n","        all_label_ids = torch.tensor([f.label_id for f in features],dtype=torch.long)\n","        dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n","        return dataset"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Writing pybert/io/bert_processor\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qJ7g8dYOA29z","colab_type":"code","colab":{}},"source":["%pycat pybert/train/trainer.py"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3EcUSBDOjyIl","colab_type":"code","outputId":"c928aada-165e-45eb-eee0-5d8d073bfb30","executionInfo":{"status":"ok","timestamp":1579334805602,"user_tz":-330,"elapsed":1373,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%%writefile pybert/train/trainer.py\n","import torch\n","import copy\n","from ..callback.progressbar import ProgressBar\n","from ..common.tools import restore_checkpoint,model_device\n","from ..common.tools import summary\n","from ..common.tools import seed_everything\n","from ..common.tools import AverageMeter\n","from torch.nn.utils import clip_grad_norm_\n","\n","class Trainer(object):\n","    def __init__(self,n_gpu,\n","                 model,\n","                 epochs,\n","                 logger,\n","                 criterion,\n","                 optimizer,\n","                 lr_scheduler,\n","                 early_stopping,\n","                 epoch_metrics,\n","                 batch_metrics,\n","                 gradient_accumulation_steps,\n","                 grad_clip = 0.0,\n","                 verbose = 1,\n","                 fp16 = None,\n","                 resume_path = None,\n","                 training_monitor = None,\n","                 model_checkpoint = None\n","                 ):\n","        self.start_epoch = 1\n","        self.global_step = 0\n","        self.n_gpu = n_gpu\n","        self.model = model\n","        self.epochs = epochs\n","        self.logger =logger\n","        self.fp16 = fp16\n","        self.grad_clip = grad_clip\n","        self.verbose = verbose\n","        self.criterion = criterion\n","        self.optimizer = optimizer\n","        self.lr_scheduler = lr_scheduler\n","        self.early_stopping = early_stopping\n","        self.epoch_metrics = epoch_metrics\n","        self.batch_metrics = batch_metrics\n","        self.model_checkpoint = model_checkpoint\n","        self.training_monitor = training_monitor\n","        self.gradient_accumulation_steps = gradient_accumulation_steps\n","        self.model, self.device = model_device(n_gpu = self.n_gpu, model=self.model)\n","        if self.fp16:\n","            try:\n","                from apex import amp\n","            except ImportError:\n","                raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n","\n","        if resume_path:\n","            self.logger.info(f\"\\nLoading checkpoint: {resume_path}\")\n","            resume_dict = torch.load(resume_path / 'checkpoint_info.bin')\n","            best = resume_dict['epoch']\n","            self.start_epoch = resume_dict['epoch']\n","            if self.model_checkpoint:\n","                self.model_checkpoint.best = best\n","            self.logger.info(f\"\\nCheckpoint '{resume_path}' and epoch {self.start_epoch} loaded\")\n","\n","    def epoch_reset(self):\n","        self.outputs = []\n","        self.targets = []\n","        self.result = {}\n","        for metric in self.epoch_metrics:\n","            metric.reset()\n","\n","    def batch_reset(self):\n","        self.info = {}\n","        for metric in self.batch_metrics:\n","            metric.reset()\n","\n","    def save_info(self,epoch,best):\n","        model_save = self.model.module if hasattr(self.model, 'module') else self.model\n","        state = {\"model\":model_save,\n","                 'epoch':epoch,\n","                 'best':best}\n","        return state\n","\n","    def valid_epoch(self,data):\n","        pbar = ProgressBar(n_total=len(data))\n","        self.epoch_reset()\n","        self.model.eval()\n","        h_ = torch.zeros(1,2)\n","        with torch.no_grad():\n","            for step, batch in enumerate(data):\n","                batch = tuple(t.to(self.device) for t in batch)\n","                input_ids, input_mask, segment_ids, label_ids = batch\n","                # logits = self.model(input_ids, segment_ids,input_mask)\n","                # logits, h = self.model(input_ids, segment_ids,input_mask, hidden_state = h_)\n","                # print(f'\\n\\n input_ids.shape {input_ids.shape}\\n\\n')\n","                if(input_ids.shape[0] == h_.shape[1]):\n","                  logits, h = self.model(input_ids, segment_ids,input_mask, hidden_state = h_)\n","                else:\n","                  logits, h = self.model(input_ids, segment_ids,input_mask, hidden_state = None)  \n","                h_ = h.clone().detach() \n","\n","                self.outputs.append(logits.cpu().detach())\n","                self.targets.append(label_ids.cpu().detach())\n","                pbar.batch_step(step=step,info = {},bar_type='Evaluating')\n","            # print(f'\\n\\nself.outputs {self.outputs}\\n\\n')    \n","            self.outputs = torch.cat(self.outputs, dim = 0).cpu().detach()\n","            self.targets = torch.cat(self.targets, dim = 0).cpu().detach()\n","            loss = self.criterion(target = self.targets, output=self.outputs)\n","            self.result['valid_loss'] = loss.item()\n","            print(\"------------- valid result --------------\")\n","            if self.epoch_metrics:\n","                for metric in self.epoch_metrics:\n","                    metric(logits=self.outputs, target=self.targets)\n","                    value = metric.value()\n","                    if value:\n","                        self.result[f'valid_{metric.name()}'] = value\n","            if 'cuda' in str(self.device):\n","                torch.cuda.empty_cache()\n","            return self.result\n","\n","    def train_epoch(self,data):\n","        pbar = ProgressBar(n_total = len(data))\n","        tr_loss = AverageMeter()\n","        self.epoch_reset()\n","        h_ = torch.zeros(1,2)\n","        self.batch_reset()\n","        self.model.train()\n","        \n","        for step,  batch in enumerate(data):\n","            batch = tuple(t.to(self.device) for t in batch)\n","            input_ids, input_mask, segment_ids, label_ids = batch\n","            # print(f'\\n\\n input_ids.shape {input_ids.shape , h_.shape} \\n\\n')\n","\n","            if(input_ids.shape[0] == h_.shape[1]):\n","              logits, h = self.model(input_ids, segment_ids,input_mask, hidden_state = h_)\n","            else:\n","              logits, h = self.model(input_ids, segment_ids,input_mask, hidden_state = None)  \n","            h_ = h.clone().detach() \n","            # logits = self.model(input_ids, segment_ids,input_mask)\n","            loss = self.criterion(output=logits,target=label_ids)\n","            if len(self.n_gpu) >= 2:\n","                loss = loss.mean()\n","            if self.gradient_accumulation_steps > 1:\n","                loss = loss / self.gradient_accumulation_steps\n","            if self.fp16:\n","                with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n","                    scaled_loss.backward()\n","                clip_grad_norm_(amp.master_params(self.optimizer), self.grad_clip)\n","            else:\n","                # loss.backward(retain_graph=True)\n","                loss.backward()\n","                clip_grad_norm_(self.model.parameters(), self.grad_clip)\n","            if (step + 1) % self.gradient_accumulation_steps == 0:\n","                self.lr_scheduler.step()\n","                self.optimizer.step()\n","                self.optimizer.zero_grad()\n","                self.global_step += 1\n","            if self.batch_metrics:\n","                for metric in self.batch_metrics:\n","                    metric(logits = logits,target = label_ids)\n","                    self.info[metric.name()] = metric.value()\n","            with torch.no_grad():\n","              self.info['loss'] = loss.item()\n","              tr_loss.update(loss.item(),n = 1)\n","            if self.verbose >= 1:\n","                pbar.batch_step(step= step,info = self.info,bar_type='Training')\n","            self.outputs.append(logits.cpu().detach())\n","            self.targets.append(label_ids.cpu().detach())\n","        print(\"\\n------------- train result --------------\")\n","        # epoch metric\n","        self.outputs = torch.cat(self.outputs, dim =0).cpu().detach()\n","        self.targets = torch.cat(self.targets, dim =0).cpu().detach()\n","        self.result['loss'] = tr_loss.avg\n","        if self.epoch_metrics:\n","            for metric in self.epoch_metrics:\n","                metric(logits=self.outputs, target=self.targets)\n","                value = metric.value()\n","                if value:\n","                    self.result[f'{metric.name()}'] = value\n","        if \"cuda\" in str(self.device):\n","            torch.cuda.empty_cache()\n","        return self.result\n","\n","    def train(self,train_data,valid_data,seed):\n","        seed_everything(seed)\n","        print(\"model summary info: \")\n","        for step, (input_ids, input_mask, segment_ids, label_ids) in enumerate(train_data):\n","            input_ids = input_ids.to(self.device)\n","            input_mask = input_mask.to(self.device)\n","            segment_ids = segment_ids.to(self.device)\n","            summary(self.model,*(input_ids, segment_ids,input_mask),show_input=True)\n","            break\n","\n","        # ***************************************************************\n","        for epoch in range(self.start_epoch,self.start_epoch+self.epochs):\n","            self.logger.info(f\"Epoch {epoch}/{self.epochs}\")\n","            train_log = self.train_epoch(train_data)\n","            valid_log = self.valid_epoch(valid_data)\n","            # print(f'  valid_log {valid_log}\\n\\n')\n","            logs = dict(train_log,**valid_log)\n","            # logs = dict(train_log)\n","            show_info = f'\\nEpoch: {epoch} - ' + \"-\".join([f' {key}: {value:.4f} ' for key,value in logs.items()])\n","            self.logger.info(show_info)\n","\n","            # save\n","            if self.training_monitor:\n","                self.training_monitor.epoch_step(logs) \n","\n","            # save model\n","            # if self.model_checkpoint:\n","            #     state = self.save_info(epoch,best=logs['valid_loss'])\n","            #     self.model_checkpoint.bert_epoch_step(current=logs[self.model_checkpoint.monitor],state = state)\n","\n","            if epoch%10 ==0:\n","                state = self.save_info(epoch,best=logs['valid_loss'])\n","                self.model_checkpoint.bert_epoch_step(current=logs[self.model_checkpoint.monitor],state = state)\n","\n","\n","            # early_stopping\n","            if self.early_stopping:\n","                self.early_stopping.epoch_step(epoch=epoch, current=logs[self.early_stopping.monitor])\n","                if self.early_stopping.stop_training:\n","                    break\n"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Overwriting pybert/train/trainer.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OZsqSMBbj8Io","colab_type":"code","outputId":"31ad5f8a-c4cf-49f7-9877-bff03b5bd6a9","executionInfo":{"status":"ok","timestamp":1577437158933,"user_tz":-330,"elapsed":2491,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":263}},"source":["!wget https://github-production-release-asset-2e65be.s3.amazonaws.com/167883658/d9fd1200-7d44-11e9-90e0-521fb735d8fd?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20191227%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20191227T083809Z&X-Amz-Expires=300&X-Amz-Signature=225ff6130bf73106a1acdb0b2abd178825f00948c67729057492ab8bc9420a2b&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dbiobert_v1.0_pubmed_pmc.tar.gz&response-content-type=application%2Foctet-stream"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/bin/bash: X-Amz-SignedHeaders=host: command not found\n","/bin/bash: response-content-disposition=attachment%3B%20filename%3Dbiobert_v1.0_pubmed_pmc.tar.gz: command not found\n","/bin/bash: response-content-type=application%2Foctet-stream: command not found\n","/bin/bash: X-Amz-Expires=300: command not found\n","/bin/bash: X-Amz-Signature=225ff6130bf73106a1acdb0b2abd178825f00948c67729057492ab8bc9420a2b: command not found\n","/bin/bash: X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20191227%2Fus-east-1%2Fs3%2Faws4_request: command not found\n","--2019-12-27 08:56:10--  https://github-production-release-asset-2e65be.s3.amazonaws.com/167883658/d9fd1200-7d44-11e9-90e0-521fb735d8fd?X-Amz-Algorithm=AWS4-HMAC-SHA256\n","/bin/bash: X-Amz-Date=20191227T083809Z: command not found\n","Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.100.11\n","Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.100.11|:443... connected.\n","HTTP request sent, awaiting response... 400 Bad Request\n","2019-12-27 08:56:10 ERROR 400: Bad Request.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GVveyqcMlHCp","colab_type":"code","outputId":"e4a09b03-b5d9-417c-cfa9-42b134d73a1a","executionInfo":{"status":"ok","timestamp":1577437266691,"user_tz":-330,"elapsed":25767,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":315}},"source":["!wget https://github.com/naver/biobert-pretrained/releases/download/v1.0-pubmed/biobert_v1.0_pubmed.tar.gz"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2019-12-27 08:57:35--  https://github.com/naver/biobert-pretrained/releases/download/v1.0-pubmed/biobert_v1.0_pubmed.tar.gz\n","Resolving github.com (github.com)... 192.30.253.112\n","Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/167883658/bab2b480-7d45-11e9-981a-f859875a728a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20191227%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20191227T085735Z&X-Amz-Expires=300&X-Amz-Signature=98cf602830a5ebf7003f186b7af723dae2762a7bcfef7acf4ee53dec0950bf13&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dbiobert_v1.0_pubmed.tar.gz&response-content-type=application%2Foctet-stream [following]\n","--2019-12-27 08:57:35--  https://github-production-release-asset-2e65be.s3.amazonaws.com/167883658/bab2b480-7d45-11e9-981a-f859875a728a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20191227%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20191227T085735Z&X-Amz-Expires=300&X-Amz-Signature=98cf602830a5ebf7003f186b7af723dae2762a7bcfef7acf4ee53dec0950bf13&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dbiobert_v1.0_pubmed.tar.gz&response-content-type=application%2Foctet-stream\n","Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.236.19\n","Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.236.19|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 402137710 (384M) [application/octet-stream]\n","Saving to: ‘biobert_v1.0_pubmed.tar.gz’\n","\n","biobert_v1.0_pubmed 100%[===================>] 383.51M  22.0MB/s    in 23s     \n","\n","2019-12-27 08:57:58 (16.8 MB/s) - ‘biobert_v1.0_pubmed.tar.gz’ saved [402137710/402137710]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WOYVgsMolKMC","colab_type":"code","outputId":"628ea2e6-6857-4196-8e4d-8655dbfafa99","executionInfo":{"status":"ok","timestamp":1577461476662,"user_tz":-330,"elapsed":3282,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":86}},"source":["!ls\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["biobert_v1.0_pmc\t    __init__.py  requirements.txt  run_xlnet.py\n","biobert_v1.0_pmc.tar.gz     Pipfile\t run_bert1.py\t   Untitled.ipynb\n","biobert_v1.0_pubmed\t    pybert\t run_bert.py\n","biobert_v1.0_pubmed.tar.gz  README.md\t run.ipynb\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TRQh-EkqlpHB","colab_type":"code","outputId":"d31eff8a-7168-4535-c718-076d5cd8ee13","executionInfo":{"status":"ok","timestamp":1577437899525,"user_tz":-330,"elapsed":11498,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":173}},"source":["!tar -xvf biobert_v1.0_pubmed.tar.gz"],"execution_count":0,"outputs":[{"output_type":"stream","text":["./._biobert_v1.0_pubmed\n","biobert_v1.0_pubmed/\n","biobert_v1.0_pubmed/biobert_model.ckpt.meta\n","biobert_v1.0_pubmed/biobert_model.ckpt.data-00000-of-00001\n","biobert_v1.0_pubmed/biobert_model.ckpt.index\n","biobert_v1.0_pubmed/._bert_config.json\n","biobert_v1.0_pubmed/bert_config.json\n","biobert_v1.0_pubmed/._vocab.txt\n","biobert_v1.0_pubmed/vocab.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BAMxNlRjnyVS","colab_type":"code","outputId":"d43f575a-18d8-492d-da4b-7f856568c5b9","executionInfo":{"status":"ok","timestamp":1577461600232,"user_tz":-330,"elapsed":1750,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cd biobert_v1.0_pmc/"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Academics/Project/BERT/Bert-Multi-Label-Text-Classification-master/biobert_v1.0_pmc\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2uBPdPZpCciO","colab_type":"code","outputId":"276830a4-2930-45ea-f38c-b27ba7cde81e","executionInfo":{"status":"ok","timestamp":1577461612852,"user_tz":-330,"elapsed":8265,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["!ls\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["bert_config.json\t\t\tbiobert_model.ckpt.meta\n","biobert_model.ckpt.data-00000-of-00001\tpytorch_model.bin\n","biobert_model.ckpt.index\t\tvocab.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UDdOqXRMCeBS","colab_type":"code","colab":{}},"source":["!cp bert_config.json ../pybert/pretrain/bert/base-uncased/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6UR80x5KCvHa","colab_type":"code","colab":{}},"source":["!cp vocab.txt ../pybert/pretrain/bert/base-uncased/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Szkqro92DqIU","colab_type":"code","outputId":"25167625-6a24-4553-a7f5-e3af5f4c87e5","executionInfo":{"status":"ok","timestamp":1577606222519,"user_tz":-330,"elapsed":1197,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cd pybert/dataset/"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Academics/Project/BERT/Bert-Multi-Label-Text-Classification-master/pybert/dataset\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vRCTY9ZzpwA8","colab_type":"code","outputId":"5aa77858-18a9-4f07-fe15-6301af29bc3a","executionInfo":{"status":"ok","timestamp":1577606230393,"user_tz":-330,"elapsed":5080,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["!ls"],"execution_count":0,"outputs":[{"output_type":"stream","text":["__init__.py  symptoms+.csv  test1.csv  train1.csv  train.csv\n","med_map.csv  sympyoms.csv   test.csv   train2.csv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gLyazuVupxVb","colab_type":"code","colab":{}},"source":["import pandas as pd\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"97qPk5bup1OU","colab_type":"code","colab":{}},"source":["df = pd.read_hdf('pybert/dataset/train.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eW_14pBep5Zb","colab_type":"code","outputId":"999ae2c5-7cc4-42e9-db58-d1d687523b83","executionInfo":{"status":"ok","timestamp":1577609031015,"user_tz":-330,"elapsed":1284,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":363}},"source":["df"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>symptoms</th>\n","      <th>label</th>\n","      <th>med_vec</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>clinical_findings C/C.  clinical_notes nan.  ...</td>\n","      <td>[12, 6, 12, 6, 12, 9, 12]</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>clinical_findings Swelling of Knee.  clinical...</td>\n","      <td>[13, 6]</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>clinical_findings Cough with output.  clinica...</td>\n","      <td>[14]</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>clinical_findings Test.  clinical_notes nan. ...</td>\n","      <td>[6, 10, 35, 17]</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>clinical_findings High temperature.  clinical...</td>\n","      <td>[10, 7]</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>clinical_findings nan.  clinical_notes nan.  ...</td>\n","      <td>[6, 5, 37]</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>clinical_findings High Temperature.  clinical...</td>\n","      <td>[8, 4, 2]</td>\n","      <td>[0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>clinical_findings Tenderness over the supra s...</td>\n","      <td>[49]</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>clinical_findings nan.  clinical_notes nan.  ...</td>\n","      <td>[8, 51]</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>clinical_findings nan.  clinical_notes nan.  ...</td>\n","      <td>[26, 26, 38, 8, 36]</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            symptoms  ...                                            med_vec\n","0   clinical_findings C/C.  clinical_notes nan.  ...  ...  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...\n","1   clinical_findings Swelling of Knee.  clinical...  ...  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...\n","2   clinical_findings Cough with output.  clinica...  ...  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n","3   clinical_findings Test.  clinical_notes nan. ...  ...  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...\n","4   clinical_findings High temperature.  clinical...  ...  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...\n","5   clinical_findings nan.  clinical_notes nan.  ...  ...  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, ...\n","6   clinical_findings High Temperature.  clinical...  ...  [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, ...\n","7   clinical_findings Tenderness over the supra s...  ...  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n","8   clinical_findings nan.  clinical_notes nan.  ...  ...  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...\n","9   clinical_findings nan.  clinical_notes nan.  ...  ...  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...\n","\n","[10 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"diZfzSWmqMoj","colab_type":"code","outputId":"71f98c18-46cc-46dc-c4c9-d5381d0b0c40","executionInfo":{"status":"ok","timestamp":1577606556638,"user_tz":-330,"elapsed":1095,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cd ../.."],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Academics/Project/BERT/Bert-Multi-Label-Text-Classification-master\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PpRpbCp2raQb","colab_type":"code","colab":{}},"source":["%pycat pybert/train/trainer.py"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"InXggIGOriET","colab_type":"code","outputId":"36b5531a-259c-4f0f-e7ba-cd23a1df6078","executionInfo":{"status":"ok","timestamp":1577610653135,"user_tz":-330,"elapsed":1125,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%%writefile pybert/train/trainer.py\n","import torch\n","from ..callback.progressbar import ProgressBar\n","from ..common.tools import restore_checkpoint,model_device\n","from ..common.tools import summary\n","from ..common.tools import seed_everything\n","from ..common.tools import AverageMeter\n","from torch.nn.utils import clip_grad_norm_\n","\n","class Trainer(object):\n","    def __init__(self,n_gpu,\n","                 model,\n","                 epochs,\n","                 logger,\n","                 criterion,\n","                 optimizer,\n","                 lr_scheduler,\n","                 early_stopping,\n","                 epoch_metrics,\n","                 batch_metrics,\n","                 gradient_accumulation_steps,\n","                 grad_clip = 0.0,\n","                 verbose = 1,\n","                 fp16 = None,\n","                 resume_path = None,\n","                 training_monitor = None,\n","                 model_checkpoint = None\n","                 ):\n","        self.start_epoch = 1\n","        self.global_step = 0\n","        self.n_gpu = n_gpu\n","        self.model = model\n","        self.epochs = epochs\n","        self.logger =logger\n","        self.fp16 = fp16\n","        self.grad_clip = grad_clip\n","        self.verbose = verbose\n","        self.criterion = criterion\n","        self.optimizer = optimizer\n","        self.lr_scheduler = lr_scheduler\n","        self.early_stopping = early_stopping\n","        self.epoch_metrics = epoch_metrics\n","        self.batch_metrics = batch_metrics\n","        self.model_checkpoint = model_checkpoint\n","        self.training_monitor = training_monitor\n","        self.gradient_accumulation_steps = gradient_accumulation_steps\n","        self.model, self.device = model_device(n_gpu = self.n_gpu, model=self.model)\n","        if self.fp16:\n","            try:\n","                from apex import amp\n","            except ImportError:\n","                raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n","\n","        if resume_path:\n","            self.logger.info(f\"\\nLoading checkpoint: {resume_path}\")\n","            resume_dict = torch.load(resume_path / 'checkpoint_info.bin')\n","            best = resume_dict['epoch']\n","            self.start_epoch = resume_dict['epoch']\n","            if self.model_checkpoint:\n","                self.model_checkpoint.best = best\n","            self.logger.info(f\"\\nCheckpoint '{resume_path}' and epoch {self.start_epoch} loaded\")\n","\n","    def epoch_reset(self):\n","        self.outputs = []\n","        self.targets = []\n","        self.result = {}\n","        for metric in self.epoch_metrics:\n","            metric.reset()\n","\n","    def batch_reset(self):\n","        self.info = {}\n","        for metric in self.batch_metrics:\n","            metric.reset()\n","\n","    def save_info(self,epoch,best):\n","        model_save = self.model.module if hasattr(self.model, 'module') else self.model\n","        state = {\"model\":model_save,\n","                 'epoch':epoch,\n","                 'best':best}\n","        return state\n","\n","    def valid_epoch(self,data):\n","        pbar = ProgressBar(n_total=len(data))\n","        self.epoch_reset()\n","        self.model.eval()\n","        with torch.no_grad():\n","            for step, batch in enumerate(data):\n","                batch = tuple(t.to(self.device) for t in batch)\n","                input_ids, input_mask, segment_ids, label_ids = batch\n","                logits = self.model(input_ids, segment_ids,input_mask)\n","                self.outputs.append(logits.cpu().detach())\n","                self.targets.append(label_ids.cpu().detach())\n","                pbar.batch_step(step=step,info = {},bar_type='Evaluating')\n","            self.outputs = torch.cat(self.outputs, dim = 0).cpu().detach()\n","            self.targets = torch.cat(self.targets, dim = 0).cpu().detach()\n","            loss = self.criterion(target = self.targets, output=self.outputs)\n","            self.result['valid_loss'] = loss.item()\n","            print(\"------------- valid result --------------\")\n","            if self.epoch_metrics:\n","                for metric in self.epoch_metrics:\n","                    metric(logits=self.outputs, target=self.targets)\n","                    value = metric.value()\n","                    if value is not None:\n","                        self.result[f'valid_{metric.name()}'] = value\n","            if 'cuda' in str(self.device):\n","                torch.cuda.empty_cache()\n","            return self.result\n","\n","    def train_epoch(self,data):\n","        pbar = ProgressBar(n_total = len(data))\n","        tr_loss = AverageMeter()\n","        self.epoch_reset()\n","        for step,  batch in enumerate(data):\n","            self.batch_reset()\n","            self.model.train()\n","            batch = tuple(t.to(self.device) for t in batch)\n","            input_ids, input_mask, segment_ids, label_ids = batch\n","            # print(f'label_ids  {label_ids.shape}')\n","            logits = self.model(input_ids, segment_ids,input_mask)\n","            # print(f'logits  {logits.shape}')\n","            loss = self.criterion(output=logits,target=label_ids)\n","            if len(self.n_gpu) >= 2:\n","                loss = loss.mean()\n","            if self.gradient_accumulation_steps > 1:\n","                loss = loss / self.gradient_accumulation_steps\n","            if self.fp16:\n","                with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n","                    scaled_loss.backward()\n","                clip_grad_norm_(amp.master_params(self.optimizer), self.grad_clip)\n","            else:\n","                loss.backward()\n","                clip_grad_norm_(self.model.parameters(), self.grad_clip)\n","            if (step + 1) % self.gradient_accumulation_steps == 0:\n","                self.lr_scheduler.step()\n","                self.optimizer.step()\n","                self.optimizer.zero_grad()\n","                self.global_step += 1\n","            if self.batch_metrics:\n","                for metric in self.batch_metrics:\n","                    metric(logits = logits,target = label_ids)\n","                    self.info[metric.name()] = metric.value()\n","            self.info['loss'] = loss.item()\n","            tr_loss.update(loss.item(),n = 1)\n","            if self.verbose >= 1:\n","                pbar.batch_step(step= step,info = self.info,bar_type='Training')\n","            self.outputs.append(logits.cpu().detach())\n","            self.targets.append(label_ids.cpu().detach())\n","        print(\"\\n------------- train result --------------\")\n","        # epoch metric\n","        self.outputs = torch.cat(self.outputs, dim =0).cpu().detach()\n","        self.targets = torch.cat(self.targets, dim =0).cpu().detach()\n","        self.result['loss'] = tr_loss.avg\n","        if self.epoch_metrics:\n","            for metric in self.epoch_metrics:\n","                metric(logits=self.outputs, target=self.targets)\n","                value = metric.value()\n","                if value is not None:\n","                    self.result[f'{metric.name()}'] = value\n","        if \"cuda\" in str(self.device):\n","            torch.cuda.empty_cache()\n","        return self.result\n","\n","    def train(self,train_data,valid_data,seed):\n","        seed_everything(seed)\n","        print(\"model summary info: \")\n","        for step, (input_ids, input_mask, segment_ids, label_ids) in enumerate(train_data):\n","            input_ids = input_ids.to(self.device)\n","            input_mask = input_mask.to(self.device)\n","            segment_ids = segment_ids.to(self.device)\n","            summary(self.model,*(input_ids, segment_ids,input_mask),show_input=True)\n","            break\n","\n","        # ***************************************************************\n","        for epoch in range(self.start_epoch,self.start_epoch+self.epochs):\n","            self.logger.info(f\"Epoch {epoch}/{self.epochs}\")\n","            train_log = self.train_epoch(train_data)\n","            valid_log = self.valid_epoch(valid_data)\n","\n","            logs = dict(train_log,**valid_log)\n","            show_info = f'\\nEpoch: {epoch} - ' + \"-\".join([f' {key}: {value:.4f} ' for key,value in logs.items()])\n","            self.logger.info(show_info)\n","\n","            # save\n","            if self.training_monitor:\n","                # print(f'\\n\\n\\nlogs  {logs}\\n\\n\\n ')                 \n","                self.training_monitor.epoch_step(logs) \n","\n","            # save model\n","            # if self.model_checkpoint:\n","            #     state = self.save_info(epoch,best=logs['valid_loss'])\n","            #     self.model_checkpoint.bert_epoch_step(current=logs[self.model_checkpoint.monitor],state = state)\n","\n","            # early_stopping\n","            if self.early_stopping:\n","                self.early_stopping.epoch_step(epoch=epoch, current=logs[self.early_stopping.monitor])\n","                if self.early_stopping.stop_training:\n","                    break"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Overwriting pybert/train/trainer.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LcyRPNvDGjwP","colab_type":"code","colab":{}},"source":["!cp pybert/pretrain/bert/base-uncased/config.json  pybert/output/checkpoints/bert/checkpoint-epoch-20/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EZrLPEQ9HS1o","colab_type":"code","colab":{}},"source":["%pycat pybert/output/checkpoints/bert/checkpoint-epoch-20/config.json"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nz2fhTf-r9XT","colab_type":"code","colab":{}},"source":["%pycat pybert/pretrain/bert/base-uncased/config.json"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NL0AsMrsur68","colab_type":"code","outputId":"10f75706-a42a-4a81-cb1d-2b7a9f5c9df1","executionInfo":{"status":"ok","timestamp":1579341576260,"user_tz":-330,"elapsed":2665,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%%writefile pybert/pretrain/bert/base-uncased/config.json\n","{\n","  \"attention_probs_dropout_prob\": 0.01,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.01,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 28996,\n","  \"num_labels\": 200\n","}\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Overwriting pybert/pretrain/bert/base-uncased/config.json\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tEpn838-u8WF","colab_type":"code","outputId":"7b6e47c8-b1f3-49d3-f38a-e1a84b728cd1","executionInfo":{"status":"ok","timestamp":1577634330359,"user_tz":-330,"elapsed":3726,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":86}},"source":["!ls"],"execution_count":0,"outputs":[{"output_type":"stream","text":["biobert_v1.0_pmc\t    __init__.py  requirements.txt  run_xlnet.py\n","biobert_v1.0_pmc.tar.gz     Pipfile\t run_bert1.py\t   Untitled.ipynb\n","biobert_v1.0_pubmed\t    pybert\t run_bert.py\n","biobert_v1.0_pubmed.tar.gz  README.md\t run.ipynb\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9VPus1onVWYr","colab_type":"code","outputId":"b54ad758-3c9a-4e2d-f88a-40e9f166d6dc","executionInfo":{"status":"ok","timestamp":1577634337078,"user_tz":-330,"elapsed":919,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cd ..\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Academics/Project/BERT\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HKJNiRLmVYyC","colab_type":"code","outputId":"a31482da-1f69-4d85-ce0c-49b3edecd00f","executionInfo":{"status":"ok","timestamp":1577634345103,"user_tz":-330,"elapsed":3906,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":104}},"source":["ls\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["bert-base-uncased-config.json                           \u001b[0m\u001b[01;34mstatus_model2\u001b[0m/\n","\u001b[01;34mBert-Multi-Label-Text-Classification-master\u001b[0m/            uc_bert_input\n","\u001b[01;34mBert-Multi-Label-Text-Classification-master2\u001b[0m/           uc_bert_input.csv\n","Copy_of_BERT_Fine_Tuning_Sentence_Classification.ipynb  uc_bert_input.gsheet\n","metrics.csv                                             Untitled.ipynb\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qi5oqMyPVaDh","colab_type":"code","outputId":"92b81853-770a-4486-905b-bc0ee479d331","executionInfo":{"status":"ok","timestamp":1577634731100,"user_tz":-330,"elapsed":146348,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":384}},"source":["!wget -O pretrained_bert_tf.tar.gz https://www.dropbox.com/s/8armk04fu16algz/pretrained_bert_tf.tar.gz?dl=1"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2019-12-29 15:46:39--  https://www.dropbox.com/s/8armk04fu16algz/pretrained_bert_tf.tar.gz?dl=1\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.1.1, 2620:100:6016:1::a27d:101\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.1.1|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /s/dl/8armk04fu16algz/pretrained_bert_tf.tar.gz [following]\n","--2019-12-29 15:46:39--  https://www.dropbox.com/s/dl/8armk04fu16algz/pretrained_bert_tf.tar.gz\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://ucdc1a94292ce6700bfd7421f9ef.dl.dropboxusercontent.com/cd/0/get/AvJQwtnXZIiGIJMxYvIkaHscqQHAACfF26-KpLu5n59D3VoXhFfTLv8ozMBqy07PlxqLiA8dq5xwyH4lB8ETXEmKYi1P4LbqDsvshqFDMCGh9jgcpDEcwGvd9GJFWXhOu7U/file?dl=1# [following]\n","--2019-12-29 15:46:40--  https://ucdc1a94292ce6700bfd7421f9ef.dl.dropboxusercontent.com/cd/0/get/AvJQwtnXZIiGIJMxYvIkaHscqQHAACfF26-KpLu5n59D3VoXhFfTLv8ozMBqy07PlxqLiA8dq5xwyH4lB8ETXEmKYi1P4LbqDsvshqFDMCGh9jgcpDEcwGvd9GJFWXhOu7U/file?dl=1\n","Resolving ucdc1a94292ce6700bfd7421f9ef.dl.dropboxusercontent.com (ucdc1a94292ce6700bfd7421f9ef.dl.dropboxusercontent.com)... 162.125.8.6, 2620:100:6016:6::a27d:106\n","Connecting to ucdc1a94292ce6700bfd7421f9ef.dl.dropboxusercontent.com (ucdc1a94292ce6700bfd7421f9ef.dl.dropboxusercontent.com)|162.125.8.6|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 6418595961 (6.0G) [application/binary]\n","Saving to: ‘pretrained_bert_tf.tar.gz’\n","\n","pretrained_bert_tf. 100%[===================>]   5.98G  42.0MB/s    in 2m 23s  \n","\n","2019-12-29 15:49:03 (42.9 MB/s) - ‘pretrained_bert_tf.tar.gz’ saved [6418595961/6418595961]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cHD4uTDdVbr5","colab_type":"code","outputId":"09a42618-722e-4884-cb8a-53cc6cc5014d","executionInfo":{"status":"ok","timestamp":1577636394378,"user_tz":-330,"elapsed":3019,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["!ls\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["bert-base-uncased-config.json\t\t\t\tpretrained_bert_tf.tar\n","Bert-Multi-Label-Text-Classification-master\t\tstatus_model2\n","Bert-Multi-Label-Text-Classification-master2\t\tuc_bert_input\n","biobert_pretrain_output_all_notes_150000\t\tuc_bert_input.csv\n","Copy_of_BERT_Fine_Tuning_Sentence_Classification.ipynb\tuc_bert_input.gsheet\n","metrics.csv\t\t\t\t\t\tUntitled.ipynb\n","pretrained_bert_tf\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZzhnNyyzXFIR","colab_type":"code","outputId":"a3cd8393-b477-4c87-9330-07793eeb3189","executionInfo":{"status":"ok","timestamp":1577636384940,"user_tz":-330,"elapsed":3559,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["!pwd"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Academics/Project/BERT\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wdQzO5BPXUJZ","colab_type":"code","colab":{}},"source":["!gunzip  pretrained_bert_tf.tar.gz"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"URj8tTbHYYVa","colab_type":"code","outputId":"68c02ffc-f306-424a-b53a-4b88462a6568","executionInfo":{"status":"ok","timestamp":1577635679024,"user_tz":-330,"elapsed":126110,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":104}},"source":["!tar -xvf pretrained_bert_tf.tar"],"execution_count":0,"outputs":[{"output_type":"stream","text":["pretrained_bert_tf/\n","pretrained_bert_tf/biobert_pretrain_output_all_notes_150000.tar.gz\n","pretrained_bert_tf/bert_pretrain_output_all_notes_150000.tar.gz\n","pretrained_bert_tf/bert_pretrain_output_disch_100000.tar.gz\n","pretrained_bert_tf/biobert_pretrain_output_disch_100000.tar.gz\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jvv7AyaWaB4i","colab_type":"code","colab":{}},"source":["!gunzip  pretrained_bert_tf/biobert_pretrain_output_all_notes_150000.tar.gz"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zaTX9qdpbUwz","colab_type":"code","outputId":"537c0b7c-56d9-4d71-df02-2212a92e9c7c","executionInfo":{"status":"ok","timestamp":1577636183273,"user_tz":-330,"elapsed":3798,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":86}},"source":["!ls pretrained_bert_tf/\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["bert_pretrain_output_all_notes_150000.tar.gz\n","bert_pretrain_output_disch_100000.tar.gz\n","biobert_pretrain_output_all_notes_150000.tar\n","biobert_pretrain_output_disch_100000.tar.gz\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qsYbiupDcPJa","colab_type":"code","outputId":"4dc2ca4f-0b89-497c-e6ce-12b168f33789","executionInfo":{"status":"ok","timestamp":1577636241014,"user_tz":-330,"elapsed":32124,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":156}},"source":["!tar -xvf pretrained_bert_tf/biobert_pretrain_output_all_notes_150000.tar"],"execution_count":0,"outputs":[{"output_type":"stream","text":["biobert_pretrain_output_all_notes_150000/\n","biobert_pretrain_output_all_notes_150000/model.ckpt-150000.index\n","biobert_pretrain_output_all_notes_150000/model.ckpt-150000.meta\n","biobert_pretrain_output_all_notes_150000/graph.pbtxt\n","biobert_pretrain_output_all_notes_150000/bert_config.json\n","biobert_pretrain_output_all_notes_150000/vocab.txt\n","biobert_pretrain_output_all_notes_150000/pytorch_model.bin\n","biobert_pretrain_output_all_notes_150000/model.ckpt-150000.data-00000-of-00001\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JUH_287MciCL","colab_type":"code","outputId":"3b6ee1f7-a5c1-403f-cf1e-831f055e4c39","executionInfo":{"status":"ok","timestamp":1577719357500,"user_tz":-330,"elapsed":2647,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["!ls ../biobert_pretrain_output_all_notes_150000/\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["bert_config.json  model.ckpt-150000.index  vocab.txt\n","graph.pbtxt\t  model.ckpt-150000.meta\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Gi8yVbKeZnJe","colab_type":"code","outputId":"143db83a-1ca9-4950-f233-9d6525362f6a","executionInfo":{"status":"ok","timestamp":1577720036726,"user_tz":-330,"elapsed":4107,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":243}},"source":["!ls -R pybert/output/checkpoints/\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["pybert/output/checkpoints/:\n","bert  __init__.py  xlnet\n","\n","pybert/output/checkpoints/bert:\n"," checkpoint-epoch-17\t    config.json         training_args.bin\n","'checkpoint_info (1).bin'   __init__.py\n"," checkpoint_info.bin\t    pytorch_model.bin\n","\n","pybert/output/checkpoints/bert/checkpoint-epoch-17:\n","checkpoint_info.bin  config.json  pytorch_model.bin\n","\n","pybert/output/checkpoints/xlnet:\n","__init__.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"V89ydItWMp17","colab_type":"code","outputId":"c17c58ed-1939-4af6-def4-aba7639679bc","executionInfo":{"status":"ok","timestamp":1578899965417,"user_tz":-330,"elapsed":8173,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":714}},"source":["!pip install pytorch_transformers\n","\n","!pip install pytorch-pretrained-bert pytorch-nlp"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: pytorch_transformers in /usr/local/lib/python3.6/dist-packages (1.2.0)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.3.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.15.0)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.10.47)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (4.28.1)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (0.1.85)\n","Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (2019.12.20)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (0.0.38)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (2.21.0)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (0.9.4)\n","Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (0.2.1)\n","Requirement already satisfied: botocore<1.14.0,>=1.13.47 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (1.13.47)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch_transformers) (0.14.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch_transformers) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch_transformers) (7.0)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (1.24.3)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (2.8)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (2019.11.28)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->pytorch_transformers) (0.15.2)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->pytorch_transformers) (2.6.1)\n","Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.6/dist-packages (0.6.2)\n","Requirement already satisfied: pytorch-nlp in /usr/local/lib/python3.6/dist-packages (0.5.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.15.0)\n","Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.3.1)\n","Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.10.47)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2019.11.28)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n","Requirement already satisfied: botocore<1.14.0,>=1.13.47 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.13.47)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n","Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.2.1)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->pytorch-pretrained-bert) (0.15.2)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->pytorch-pretrained-bert) (2.6.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.14.0,>=1.13.47->boto3->pytorch-pretrained-bert) (1.12.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HO7dV9JlcPdV","colab_type":"code","outputId":"a304a4e9-5230-4c17-cbbc-2aef3b913f9c","executionInfo":{"status":"ok","timestamp":1578902302311,"user_tz":-330,"elapsed":12329,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#%run  run_bert.py --do_data --do_train --data_name uc --train_batch_size 64 --eval_batch_size 1024\n","!python  run_bert.py --do_test --do_lower_case --data_name uc  "],"execution_count":0,"outputs":[{"output_type":"stream","text":["Training/evaluation parameters Namespace(adam_epsilon=1e-08, arch='bert', data_name='uc', do_data=False, do_lower_case=True, do_test=True, do_train=False, epochs=6, eval_batch_size=8, eval_max_seq_len=256, fp16=False, fp16_opt_level='O1', grad_clip=1.0, gradient_accumulation_steps=1, learning_rate=2e-05, local_rank=-1, loss_scale=0, mode='min', monitor='valid_loss', n_gpu='0', resume_path='', save_best=False, seed=42, sorted=1, train_batch_size=8, train_max_seq_len=256, valid_size=0.2, warmup_proportion=0.1, weight_decay=0.01)\n","NumExpr defaulting to 2 threads.\n","\n","\n"," SYMPTOMS: C/o 1 month mid epigastric pain, burning, pain increases after food, eats at hotel for 10 years. has c/o anal itching and bloating F/U hypertension, telmisartan 40mg daily , 5 years plus, CLINICAL FINDINGS: NAD EXT: no edema , CVS: RRR no m , RS: CTA P\n","\n","\n"," SYMPTOMS: Renal Calculi + on Rx, H/O Cervical lump\n","\n","\n"," SYMPTOMS: c/o pain and instability b/l knee, DIAGNOSIS: OA B/L KNEE, ADVISE: REFFRED TO SSSIHMS PG ORTHO\n","\n","\n"," SYMPTOMS: C/O pf frequent urination since last 2 years Pain in the lower limbs since last 3 years, PAST HISTORY: Not significant, FAMILY HISTORY: Not Significant, CLINICAL FINDINGS: B/L knee crepitus +, CVS: NAD, RS: NAD, PA: NAD, CNS: NAD, DIAGNOSIS: OA Knee, ADVISE: Low salt diet\n","\n","\n"," SYMPTOMS: ULCER OVER RIGHT POLITEAL FOSSA RIGHT, DIAGNOSIS: ULCER OVER RIGHT POPLITEAL FOSSA, ADVISE: REFFRED TO GENERAL SURGERY GH PUTTAPARTHY\n","-----------------------------------------\n","targets [[-1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1]]\n","---------------------------------------------------\n","[create examples] 5/5 [==============================] 120.7us/stepSaving examples into cached file pybert/dataset/cached_test_examples_bert\n","*** Example ***\n","guid: test-0\n","tokens: [CLS] symptoms : c / o 1 month mid e ##pi ##gas ##tric pain , burning , pain increases after food , eats at hotel for 10 years . has c / o anal it ##ching and b ##loat ##ing f / u h ##yper ##tens ##ion , te ##lm ##isa ##rta ##n 40 ##m ##g daily , 5 years plus , clinical findings : na ##d ex ##t : no ed ##ema , c ##v ##s : r ##r no m , r ##s : c ##ta p [SEP]\n","input_ids: 101 8006 131 172 120 184 122 2370 2286 174 8508 11305 11048 2489 117 4968 117 2489 6986 1170 2094 117 24347 1120 3415 1111 1275 1201 119 1144 172 120 184 24443 1122 7520 1105 171 23223 1158 175 120 190 177 24312 23826 1988 117 21359 13505 15630 16242 1179 1969 1306 1403 3828 117 126 1201 4882 117 7300 9505 131 9468 1181 4252 1204 131 1185 5048 14494 117 172 1964 1116 131 187 1197 1185 182 117 187 1116 131 172 1777 185 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","[create features] 1/5 [=====>........................] - ETA: 0s*** Example ***\n","guid: test-1\n","tokens: [CLS] symptoms : re ##nal ca ##l ##cu ##li + on r ##x , h / o c ##er ##vic ##al lump [SEP]\n","input_ids: 101 8006 131 1231 7050 11019 1233 10182 2646 116 1113 187 1775 117 177 120 184 172 1200 15901 1348 16401 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","[create features] 5/5 [==============================] 1.2ms/stepSaving features into cached file pybert/dataset/cached_test_features_256_bert\n","loading configuration file pybert/output/checkpoints/bert/config.json\n","Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_labels\": 200,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"pruned_heads\": {},\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 28996\n","}\n","\n","loading weights file pybert/output/checkpoints/bert/pytorch_model.bin\n","model predicting....\n","logits.shape torch.Size([5, 200])\n","[Testing] 1/1 [==============================] 55.4ms/step\n","\n","1 CALCIUM CARBONATE 1250mg & VIT D3 250 IU\n","\n","2 PANTOPRAZOLE 40 mg\n","\n","3 MULTIVIT&MINERALS+METHYLCOBLAMIN+ANTIOXIDANTS\n","\n","4 PARACETAMOL 500mg\n","\n","5 DICLOFENAC Ointment\n","\n","6 VITAMIN B Complex\n","\n","7 RANITIDINE 150 mg\n","\n","\n","\n","\n","1 CALCIUM CARBONATE 1250mg & VIT D3 250 IU\n","\n","2 PANTOPRAZOLE 40 mg\n","\n","3 MULTIVIT&MINERALS+METHYLCOBLAMIN+ANTIOXIDANTS\n","\n","4 PARACETAMOL 500mg\n","\n","5 DICLOFENAC Ointment\n","\n","6 VITAMIN B Complex\n","\n","7 RANITIDINE 150 mg\n","\n","\n","\n","\n","1 CALCIUM CARBONATE 1250mg & VIT D3 250 IU\n","\n","2 PANTOPRAZOLE 40 mg\n","\n","3 MULTIVIT&MINERALS+METHYLCOBLAMIN+ANTIOXIDANTS\n","\n","4 PARACETAMOL 500mg\n","\n","5 DICLOFENAC Ointment\n","\n","6 VITAMIN B Complex\n","\n","7 RANITIDINE 150 mg\n","\n","\n","\n","\n","1 CALCIUM CARBONATE 1250mg & VIT D3 250 IU\n","\n","2 PANTOPRAZOLE 40 mg\n","\n","3 MULTIVIT&MINERALS+METHYLCOBLAMIN+ANTIOXIDANTS\n","\n","4 PARACETAMOL 500mg\n","\n","5 DICLOFENAC Ointment\n","\n","6 VITAMIN B Complex\n","\n","7 RANITIDINE 150 mg\n","\n","\n","\n","\n","1 CALCIUM CARBONATE 1250mg & VIT D3 250 IU\n","\n","2 PANTOPRAZOLE 40 mg\n","\n","3 MULTIVIT&MINERALS+METHYLCOBLAMIN+ANTIOXIDANTS\n","\n","4 PARACETAMOL 500mg\n","\n","5 DICLOFENAC Ointment\n","\n","6 VITAMIN B Complex\n","\n","7 RANITIDINE 150 mg\n","\n","\n","\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TKFzDls2MiKL","colab_type":"code","colab":{}},"source":["import pandas as pd\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UU7Pct321xar","colab_type":"code","colab":{}},"source":["\n","df = pd.DataFrame([\"SYMPTOMS: C/o 1 month mid epigastric pain, burning, pain increases after food, eats at hotel for 10 years. has c/o anal itching and bloating F/U hypertension, telmisartan 40mg daily , 5 years plus, CLINICAL FINDINGS: NAD EXT: no edema , CVS: RRR no m , RS: CTA P\",\n","\"SYMPTOMS: Renal Calculi + on Rx, H/O Cervical lump\",\n","\n","\"SYMPTOMS: c/o pain and instability b/l knee, DIAGNOSIS: OA B/L KNEE, ADVISE: REFFRED TO SSSIHMS PG ORTHO\",\n","\n","\n","\"SYMPTOMS: C/O pf frequent urination since last 2 years Pain in the lower limbs since last 3 years, PAST HISTORY: Not significant, FAMILY HISTORY: Not Significant, CLINICAL FINDINGS: B/L knee crepitus +, CVS: NAD, RS: NAD, PA: NAD, CNS: NAD, DIAGNOSIS: OA Knee, ADVISE: Low salt diet\",\n","\n","\n","\n","\"SYMPTOMS: ULCER OVER RIGHT POLITEAL FOSSA RIGHT, DIAGNOSIS: ULCER OVER RIGHT POPLITEAL FOSSA, ADVISE: REFFRED TO GENERAL SURGERY GH PUTTAPARTHY\",\n","\n","])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RRtYfXSH112R","colab_type":"code","colab":{}},"source":["df.to_hdf('pybert/dataset/test.csv', key='df')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"POEvqoHF16LQ","colab_type":"code","outputId":"181e676e-3480-4b3f-fa42-e5ae253bf16a","executionInfo":{"status":"ok","timestamp":1578901322477,"user_tz":-330,"elapsed":5331,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["ls -R"],"execution_count":0,"outputs":[{"output_type":"stream","text":[".:\n","\u001b[0m\u001b[01;34mbiobert_v1.0_pmc\u001b[0m/     Pipfile    requirements.txt  run.ipynb\n","\u001b[01;34mbiobert_v1.0_pubmed\u001b[0m/  \u001b[01;34mpybert\u001b[0m/    run_bert1.py      run_xlnet.py\n","__init__.py           README.md  run_bert.py       Untitled.ipynb\n","\n","./biobert_v1.0_pmc:\n","bert_config.json  biobert_model.ckpt.index  biobert_model.ckpt.meta  vocab.txt\n","\n","./biobert_v1.0_pubmed:\n","bert_config.json  biobert_model.ckpt.index  biobert_model.ckpt.meta  vocab.txt\n","\n","./pybert:\n","\u001b[01;34mcallback\u001b[0m/  \u001b[01;34mconfigs\u001b[0m/  __init__.py  \u001b[01;34mmodel\u001b[0m/   \u001b[01;34mpreprocessing\u001b[0m/  \u001b[01;34m__pycache__\u001b[0m/  \u001b[01;34mtrain\u001b[0m/\n","\u001b[01;34mcommon\u001b[0m/    \u001b[01;34mdataset\u001b[0m/  \u001b[01;34mio\u001b[0m/          \u001b[01;34moutput\u001b[0m/  \u001b[01;34mpretrain\u001b[0m/       \u001b[01;34mtest\u001b[0m/\n","\n","./pybert/callback:\n","earlystopping.py  lrscheduler.py      optimizater.py  \u001b[01;34m__pycache__\u001b[0m/\n","__init__.py       modelcheckpoint.py  progressbar.py  trainingmonitor.py\n","\n","./pybert/callback/__pycache__:\n","__init__.cpython-36.pyc         progressbar.cpython-36.pyc\n","modelcheckpoint.cpython-36.pyc  trainingmonitor.cpython-36.pyc\n","\n","./pybert/common:\n","\u001b[01;34m__pycache__\u001b[0m/  tools.py\n","\n","./pybert/common/__pycache__:\n","tools.cpython-36.pyc\n","\n","./pybert/configs:\n","basic_config.py  __init__.py  \u001b[01;34m__pycache__\u001b[0m/\n","\n","./pybert/configs/__pycache__:\n","basic_config.cpython-36.pyc  __init__.cpython-36.pyc\n","\n","./pybert/dataset:\n","cached_test_examples_bert       med_map.csv               train1.csv\n","cached_test_features_256_bert   small_symptoms_plus1.csv  train2.csv\n","cached_train_examples_bert      small_symptoms_plus.csv   train.csv\n","cached_train_features_256_bert  sympyoms.csv              uc.train.pkl\n","cached_valid_examples_bert      test1.csv                 uc.valid.pkl\n","cached_valid_features_256_bert  test2.csv\n","__init__.py                     test.csv\n","\n","./pybert/io:\n","bert_processor     __init__.py   task_data.py   xlnet_processor.py\n","bert_processor.py  \u001b[01;34m__pycache__\u001b[0m/  vocabulary.py\n","\n","./pybert/io/__pycache__:\n","bert_processor.cpython-36.pyc  task_data.cpython-36.pyc\n","__init__.cpython-36.pyc\n","\n","./pybert/model:\n","__init__.py  \u001b[01;34mnn\u001b[0m/  \u001b[01;34m__pycache__\u001b[0m/\n","\n","./pybert/model/nn:\n","bert_for_multi_label.py  __init__.py  \u001b[01;34m__pycache__\u001b[0m/  xlnet_for_multi_label.py\n","\n","./pybert/model/nn/__pycache__:\n","bert_for_multi_label.cpython-36.pyc  __init__.cpython-36.pyc\n","\n","./pybert/model/__pycache__:\n","__init__.cpython-36.pyc\n","\n","./pybert/output:\n","\u001b[01;34mcheckpoints\u001b[0m/  \u001b[01;34membedding\u001b[0m/  \u001b[01;34mfeature\u001b[0m/  \u001b[01;34mfigure\u001b[0m/  __init__.py  \u001b[01;34mlog\u001b[0m/  \u001b[01;34mresult\u001b[0m/\n","\n","./pybert/output/checkpoints:\n","\u001b[01;34mbert\u001b[0m/  __init__.py  \u001b[01;34mxlnet\u001b[0m/\n","\n","./pybert/output/checkpoints/bert:\n"," \u001b[01;34mcheckpoint-epoch-10\u001b[0m/   \u001b[01;34mcheckpoint-epoch-50\u001b[0m/       checkpoint_info.bin\n"," \u001b[01;34mcheckpoint-epoch-17\u001b[0m/   \u001b[01;34mcheckpoint-epoch-60\u001b[0m/       config.json\n"," \u001b[01;34mcheckpoint-epoch-20\u001b[0m/   \u001b[01;34mcheckpoint-epoch-70\u001b[0m/       __init__.py\n"," \u001b[01;34mcheckpoint-epoch-30\u001b[0m/   \u001b[01;34mcheckpoint-epoch-80\u001b[0m/       pytorch_model.bin\n"," \u001b[01;34mcheckpoint-epoch-40\u001b[0m/  'checkpoint_info (1).bin'   training_args.bin\n","\n","./pybert/output/checkpoints/bert/checkpoint-epoch-10:\n","checkpoint_info.bin  config.json  pytorch_model.bin\n","\n","./pybert/output/checkpoints/bert/checkpoint-epoch-17:\n","checkpoint_info.bin  config.json  pytorch_model.bin\n","\n","./pybert/output/checkpoints/bert/checkpoint-epoch-20:\n","checkpoint_info.bin  config.json  pytorch_model.bin\n","\n","./pybert/output/checkpoints/bert/checkpoint-epoch-30:\n","checkpoint_info.bin  config.json  pytorch_model.bin\n","\n","./pybert/output/checkpoints/bert/checkpoint-epoch-40:\n","checkpoint_info.bin  config.json  pytorch_model.bin\n","\n","./pybert/output/checkpoints/bert/checkpoint-epoch-50:\n","checkpoint_info.bin  config.json  pytorch_model.bin\n","\n","./pybert/output/checkpoints/bert/checkpoint-epoch-60:\n","checkpoint_info.bin  config.json  pytorch_model.bin\n","\n","./pybert/output/checkpoints/bert/checkpoint-epoch-70:\n","checkpoint_info.bin  config.json  pytorch_model.bin\n","\n","./pybert/output/checkpoints/bert/checkpoint-epoch-80:\n","checkpoint_info.bin  config.json  pytorch_model.bin\n","\n","./pybert/output/checkpoints/xlnet:\n","__init__.py\n","\n","./pybert/output/embedding:\n","__init__.py\n","\n","./pybert/output/feature:\n","__init__.py\n","\n","./pybert/output/figure:\n","bert_ACC.png       bert_LOSS.png    bert_training_monitor.json\n","bert_ACCURACY.png  bert_RECALL.png  __init__.py\n","\n","./pybert/output/log:\n","bert.log  __init__.py\n","\n","./pybert/output/result:\n","__init__.py\n","\n","./pybert/preprocessing:\n","augmentation.py  __init__.py  preprocessor.py  \u001b[01;34m__pycache__\u001b[0m/\n","\n","./pybert/preprocessing/__pycache__:\n","__init__.cpython-36.pyc  preprocessor.cpython-36.pyc\n","\n","./pybert/pretrain:\n","\u001b[01;34mbert\u001b[0m/  __init__.py  \u001b[01;34mxlnet\u001b[0m/\n","\n","./pybert/pretrain/bert:\n","\u001b[01;34mbase-uncased\u001b[0m/\n","\n","./pybert/pretrain/bert/base-uncased:\n","bert_vocab.txt  config.json  __init__.py  pytorch_model1.bin  pytorch_model.bin\n","\n","./pybert/pretrain/xlnet:\n","\u001b[01;34mbase-cased\u001b[0m/\n","\n","./pybert/pretrain/xlnet/base-cased:\n","__init__.py\n","\n","./pybert/__pycache__:\n","__init__.cpython-36.pyc\n","\n","./pybert/test:\n","__init__.py  predictor1.py  predictor.py  \u001b[01;34m__pycache__\u001b[0m/\n","\n","./pybert/test/__pycache__:\n","__init__.cpython-36.pyc  predictor.cpython-36.pyc\n","\n","./pybert/train:\n","__init__.py  losses.py  metrics.py  \u001b[01;34m__pycache__\u001b[0m/  trainer.py\n","\n","./pybert/train/__pycache__:\n","__init__.cpython-36.pyc  metrics.cpython-36.pyc\n","losses.cpython-36.pyc    trainer.cpython-36.pyc\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"n7K8yuhH2Vxp","colab_type":"code","outputId":"b4d69246-ff50-4a47-afc7-ea7bf3c1d9eb","executionInfo":{"status":"ok","timestamp":1578901919898,"user_tz":-330,"elapsed":2367,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":112}},"source":["pd.DataFrame(['Sairam','ram'])"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Sairam</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ram</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        0\n","0  Sairam\n","1     ram"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"cIb64FDt407C","colab_type":"code","outputId":"c619f0c5-966f-4f24-c04e-005720c9b99d","executionInfo":{"status":"ok","timestamp":1578977830759,"user_tz":-330,"elapsed":5835,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["ls"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\u001b[0m\u001b[01;34mbiobert_v1.0_pmc\u001b[0m/     Pipfile    requirements.txt  run.ipynb\n","\u001b[01;34mbiobert_v1.0_pubmed\u001b[0m/  \u001b[01;34mpybert\u001b[0m/    run_bert1.py      run_xlnet.py\n","__init__.py           README.md  run_bert.py       Untitled.ipynb\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8mZ4KcfIaFFl","colab_type":"code","outputId":"716e39fd-4b57-4bc4-e27e-ee9d3369d632","executionInfo":{"status":"ok","timestamp":1578977837651,"user_tz":-330,"elapsed":1436,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cd  .."],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Academics/Project/BERT\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yz82hyKgaHqc","colab_type":"code","outputId":"0825ec9d-e81c-48a9-bd0e-f06442216d23","executionInfo":{"status":"ok","timestamp":1578977843594,"user_tz":-330,"elapsed":5143,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["ls"],"execution_count":0,"outputs":[{"output_type":"stream","text":["bert-base-uncased-config.json                           \u001b[0m\u001b[01;34mpretrained_bert_tf\u001b[0m/\n","\u001b[01;34mBert-Multi-Label-Text-Classification-master\u001b[0m/            \u001b[01;34mstatus_model2\u001b[0m/\n","\u001b[01;34mBert-Multi-Label-Text-Classification-master2\u001b[0m/           uc_bert_input\n","\u001b[01;34mbiobert_pretrain_output_all_notes_150000\u001b[0m/               uc_bert_input.csv\n","Copy_of_BERT_Fine_Tuning_Sentence_Classification.ipynb  uc_bert_input.gsheet\n","metrics.csv                                             Untitled.ipynb\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Luk8TCrtaIyM","colab_type":"code","outputId":"6db1b804-0df6-453c-f1ea-11251b373dc5","executionInfo":{"status":"ok","timestamp":1578977937852,"user_tz":-330,"elapsed":46419,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":332}},"source":["!wget -O pretrained_bert_tf.tar.gz https://www.dropbox.com/s/8armk04fu16algz/pretrained_bert_tf.tar.gz?dl=1"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2020-01-14 04:55:36--  https://www.dropbox.com/s/8armk04fu16algz/pretrained_bert_tf.tar.gz?dl=1\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.1, 2620:100:6021:1::a27d:4101\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.1|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /s/dl/8armk04fu16algz/pretrained_bert_tf.tar.gz [following]\n","--2020-01-14 04:55:36--  https://www.dropbox.com/s/dl/8armk04fu16algz/pretrained_bert_tf.tar.gz\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uc1f9271ea58b0fb34bdbaf53fdf.dl.dropboxusercontent.com/cd/0/get/AwKL-YMT5mzFml1jzNFoofhYELfYmXPd9feD8cbbt0LF23IrkUwnFDMne4Wb8mImtRAX4HS2u6pUyvnXMABcwnFOefPwb5Rqen8wQTrHJh6Ryfs9MLjXG4YpprLImkJP21s/file?dl=1# [following]\n","--2020-01-14 04:55:37--  https://uc1f9271ea58b0fb34bdbaf53fdf.dl.dropboxusercontent.com/cd/0/get/AwKL-YMT5mzFml1jzNFoofhYELfYmXPd9feD8cbbt0LF23IrkUwnFDMne4Wb8mImtRAX4HS2u6pUyvnXMABcwnFOefPwb5Rqen8wQTrHJh6Ryfs9MLjXG4YpprLImkJP21s/file?dl=1\n","Resolving uc1f9271ea58b0fb34bdbaf53fdf.dl.dropboxusercontent.com (uc1f9271ea58b0fb34bdbaf53fdf.dl.dropboxusercontent.com)... 162.125.65.6, 2620:100:6021:6::a27d:4106\n","Connecting to uc1f9271ea58b0fb34bdbaf53fdf.dl.dropboxusercontent.com (uc1f9271ea58b0fb34bdbaf53fdf.dl.dropboxusercontent.com)|162.125.65.6|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 6418595961 (6.0G) [application/binary]\n","Saving to: ‘pretrained_bert_tf.tar.gz’\n","\n","tar.gz               32%[=====>              ]   1.96G  59.8MB/s    eta 84s    ^C\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QmyDGWQ-aLwH","colab_type":"code","colab":{}},"source":["!cd .."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YcEA1piAa5x8","colab_type":"code","outputId":"6751ee4a-0248-42aa-c0fa-9f00e1fdbcd9","executionInfo":{"status":"ok","timestamp":1578977970055,"user_tz":-330,"elapsed":2126,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cd .."],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Academics/Project\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xRo8cm9ea75d","colab_type":"code","outputId":"777b9100-b5f6-46c7-ecb4-61f4c8d1a61c","executionInfo":{"status":"ok","timestamp":1578977994808,"user_tz":-330,"elapsed":5286,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":243}},"source":["ls"],"execution_count":0,"outputs":[{"output_type":"stream","text":["bert-base-uncased-config.json\n","\u001b[0m\u001b[01;34mBert-Multi-Label-Text-Classification-master\u001b[0m/\n","\u001b[01;34mBert-Multi-Label-Text-Classification-master2\u001b[0m/\n","\u001b[01;34mbiobert_pretrain_output_all_notes_150000\u001b[0m/\n","Copy_of_BERT_Fine_Tuning_Sentence_Classification.ipynb\n","metrics.csv\n","\u001b[01;34mpretrained_bert_tf\u001b[0m/\n","pretrained_bert_tf.tar.gz\n","\u001b[01;34mstatus_model2\u001b[0m/\n","uc_bert_input\n","uc_bert_input.csv\n","uc_bert_input.gsheet\n","Untitled.ipynb\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HFwyQrR7a90d","colab_type":"code","outputId":"a69c467a-5a21-4bb0-92f6-0f92ac8e6a14","executionInfo":{"status":"ok","timestamp":1578977988997,"user_tz":-330,"elapsed":2563,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cd BERT"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Academics/Project/BERT\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aiRmSK7ybAa0","colab_type":"code","outputId":"0013aecc-9e11-4f8d-9b5c-3dfdafaf1d0a","executionInfo":{"status":"ok","timestamp":1578978019864,"user_tz":-330,"elapsed":2435,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cd pretrained_bert_tf/"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Academics/Project/BERT/pretrained_bert_tf\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5CIOeb97bH_s","colab_type":"code","outputId":"4a5c825a-dd27-4870-fcb8-b7ce4906e53f","executionInfo":{"status":"ok","timestamp":1578978030288,"user_tz":-330,"elapsed":4404,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["ls"],"execution_count":0,"outputs":[{"output_type":"stream","text":["biobert_pretrain_output_all_notes_150000.tar\n","biobert_pretrain_output_disch_100000.tar.gz\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Wii6u3-ebJ3T","colab_type":"code","colab":{}},"source":["!tar -xf biobert_pretrain_output_all_notes_150000.tar"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fWivVYfkcLPp","colab_type":"code","colab":{}},"source":["# ls -l biobert_pretrain_output_all_notes_150000/\n","!cp biobert_pretrain_output_all_notes_150000/pytorch_model.bin ../Bert-Multi-Label-Text-Classification-master/pybert/pretrain/bert/base-uncased/\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"52o-flwQbbMs","colab_type":"code","outputId":"4deb283e-b03b-4d86-a62a-ea3e0e549dfd","executionInfo":{"status":"ok","timestamp":1578978119041,"user_tz":-330,"elapsed":3960,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!tar --help"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Usage: tar [OPTION...] [FILE]...\n","GNU 'tar' saves many files together into a single tape or disk archive, and can\n","restore individual files from the archive.\n","\n","Examples:\n","  tar -cf archive.tar foo bar  # Create archive.tar from files foo and bar.\n","  tar -tvf archive.tar         # List all files in archive.tar verbosely.\n","  tar -xf archive.tar          # Extract all files from archive.tar.\n","\n"," Local file name selection:\n","\n","      --add-file=FILE        add given FILE to the archive (useful if its name\n","                             starts with a dash)\n","  -C, --directory=DIR        change to directory DIR\n","      --exclude=PATTERN      exclude files, given as a PATTERN\n","      --exclude-backups      exclude backup and lock files\n","      --exclude-caches       exclude contents of directories containing\n","                             CACHEDIR.TAG, except for the tag file itself\n","      --exclude-caches-all   exclude directories containing CACHEDIR.TAG\n","      --exclude-caches-under exclude everything under directories containing\n","                             CACHEDIR.TAG\n","      --exclude-ignore=FILE  read exclude patterns for each directory from\n","                             FILE, if it exists\n","      --exclude-ignore-recursive=FILE\n","                             read exclude patterns for each directory and its\n","                             subdirectories from FILE, if it exists\n","      --exclude-tag=FILE     exclude contents of directories containing FILE,\n","                             except for FILE itself\n","      --exclude-tag-all=FILE exclude directories containing FILE\n","      --exclude-tag-under=FILE   exclude everything under directories\n","                             containing FILE\n","      --exclude-vcs          exclude version control system directories\n","      --exclude-vcs-ignores  read exclude patterns from the VCS ignore files\n","      --no-null              disable the effect of the previous --null option\n","      --no-recursion         avoid descending automatically in directories\n","      --no-unquote           do not unquote input file or member names\n","      --no-verbatim-files-from   -T treats file names starting with dash as\n","                             options (default)\n","      --null                 -T reads null-terminated names; implies\n","                             --verbatim-files-from\n","      --recursion            recurse into directories (default)\n","  -T, --files-from=FILE      get names to extract or create from FILE\n","      --unquote              unquote input file or member names (default)\n","      --verbatim-files-from  -T reads file names verbatim (no option handling)\n","  -X, --exclude-from=FILE    exclude patterns listed in FILE\n","\n"," File name matching options (affect both exclude and include patterns):\n","\n","      --anchored             patterns match file name start\n","      --ignore-case          ignore case\n","      --no-anchored          patterns match after any '/' (default for\n","                             exclusion)\n","      --no-ignore-case       case sensitive matching (default)\n","      --no-wildcards         verbatim string matching\n","      --no-wildcards-match-slash   wildcards do not match '/'\n","      --wildcards            use wildcards (default for exclusion)\n","      --wildcards-match-slash   wildcards match '/' (default for exclusion)\n","\n"," Main operation mode:\n","\n","  -A, --catenate, --concatenate   append tar files to an archive\n","  -c, --create               create a new archive\n","  -d, --diff, --compare      find differences between archive and file system\n","      --delete               delete from the archive (not on mag tapes!)\n","  -r, --append               append files to the end of an archive\n","  -t, --list                 list the contents of an archive\n","      --test-label           test the archive volume label and exit\n","  -u, --update               only append files newer than copy in archive\n","  -x, --extract, --get       extract files from an archive\n","\n"," Operation modifiers:\n","\n","      --check-device         check device numbers when creating incremental\n","                             archives (default)\n","  -g, --listed-incremental=FILE   handle new GNU-format incremental backup\n","  -G, --incremental          handle old GNU-format incremental backup\n","      --hole-detection=TYPE  technique to detect holes\n","      --ignore-failed-read   do not exit with nonzero on unreadable files\n","      --level=NUMBER         dump level for created listed-incremental archive\n","  -n, --seek                 archive is seekable\n","      --no-check-device      do not check device numbers when creating\n","                             incremental archives\n","      --no-seek              archive is not seekable\n","      --occurrence[=NUMBER]  process only the NUMBERth occurrence of each file\n","                             in the archive; this option is valid only in\n","                             conjunction with one of the subcommands --delete,\n","                             --diff, --extract or --list and when a list of\n","                             files is given either on the command line or via\n","                             the -T option; NUMBER defaults to 1\n","      --sparse-version=MAJOR[.MINOR]\n","                             set version of the sparse format to use (implies\n","                             --sparse)\n","  -S, --sparse               handle sparse files efficiently\n","\n"," Overwrite control:\n","\n","  -k, --keep-old-files       don't replace existing files when extracting,\n","                             treat them as errors\n","      --keep-directory-symlink   preserve existing symlinks to directories when\n","                             extracting\n","      --keep-newer-files     don't replace existing files that are newer than\n","                             their archive copies\n","      --no-overwrite-dir     preserve metadata of existing directories\n","      --one-top-level[=DIR]  create a subdirectory to avoid having loose files\n","                             extracted\n","      --overwrite            overwrite existing files when extracting\n","      --overwrite-dir        overwrite metadata of existing directories when\n","                             extracting (default)\n","      --recursive-unlink     empty hierarchies prior to extracting directory\n","      --remove-files         remove files after adding them to the archive\n","      --skip-old-files       don't replace existing files when extracting,\n","                             silently skip over them\n","  -U, --unlink-first         remove each file prior to extracting over it\n","  -W, --verify               attempt to verify the archive after writing it\n","\n"," Select output stream:\n","\n","      --ignore-command-error ignore exit codes of children\n","      --no-ignore-command-error   treat non-zero exit codes of children as\n","                             error\n","  -O, --to-stdout            extract files to standard output\n","      --to-command=COMMAND   pipe extracted files to another program\n","\n"," Handling of file attributes:\n","\n","      --atime-preserve[=METHOD]   preserve access times on dumped files, either\n","                             by restoring the times after reading\n","                             (METHOD='replace'; default) or by not setting the\n","                             times in the first place (METHOD='system')\n","      --clamp-mtime          only set time when the file is more recent than\n","                             what was given with --mtime\n","      --delay-directory-restore   delay setting modification times and\n","                             permissions of extracted directories until the end\n","                             of extraction\n","      --group=NAME           force NAME as group for added files\n","      --group-map=FILE       use FILE to map file owner GIDs and names\n","      --mode=CHANGES         force (symbolic) mode CHANGES for added files\n","      --mtime=DATE-OR-FILE   set mtime for added files from DATE-OR-FILE\n","  -m, --touch                don't extract file modified time\n","      --no-delay-directory-restore\n","                             cancel the effect of --delay-directory-restore\n","                             option\n","      --no-same-owner        extract files as yourself (default for ordinary\n","                             users)\n","      --no-same-permissions  apply the user's umask when extracting permissions\n","                             from the archive (default for ordinary users)\n","      --numeric-owner        always use numbers for user/group names\n","      --owner=NAME           force NAME as owner for added files\n","      --owner-map=FILE       use FILE to map file owner UIDs and names\n","  -p, --preserve-permissions, --same-permissions\n","                             extract information about file permissions\n","                             (default for superuser)\n","      --same-owner           try extracting files with the same ownership as\n","                             exists in the archive (default for superuser)\n","  -s, --preserve-order, --same-order\n","                             member arguments are listed in the same order as\n","                             the files in the archive\n","      --sort=ORDER           directory sorting order: none (default), name or\n","                             inode\n","\n"," Handling of extended file attributes:\n","\n","      --acls                 Enable the POSIX ACLs support\n","      --no-acls              Disable the POSIX ACLs support\n","      --no-selinux           Disable the SELinux context support\n","      --no-xattrs            Disable extended attributes support\n","      --selinux              Enable the SELinux context support\n","      --xattrs               Enable extended attributes support\n","      --xattrs-exclude=MASK  specify the exclude pattern for xattr keys\n","      --xattrs-include=MASK  specify the include pattern for xattr keys\n","\n"," Device selection and switching:\n","\n","  -f, --file=ARCHIVE         use archive file or device ARCHIVE\n","      --force-local          archive file is local even if it has a colon\n","  -F, --info-script=NAME, --new-volume-script=NAME\n","                             run script at end of each tape (implies -M)\n","  -L, --tape-length=NUMBER   change tape after writing NUMBER x 1024 bytes\n","  -M, --multi-volume         create/list/extract multi-volume archive\n","      --rmt-command=COMMAND  use given rmt COMMAND instead of rmt\n","      --rsh-command=COMMAND  use remote COMMAND instead of rsh\n","      --volno-file=FILE      use/update the volume number in FILE\n","\n"," Device blocking:\n","\n","  -b, --blocking-factor=BLOCKS   BLOCKS x 512 bytes per record\n","  -B, --read-full-records    reblock as we read (for 4.2BSD pipes)\n","  -i, --ignore-zeros         ignore zeroed blocks in archive (means EOF)\n","      --record-size=NUMBER   NUMBER of bytes per record, multiple of 512\n","\n"," Archive format selection:\n","\n","  -H, --format=FORMAT        create archive of the given format\n","\n"," FORMAT is one of the following:\n","\n","    gnu                      GNU tar 1.13.x format\n","    oldgnu                   GNU format as per tar <= 1.12\n","    pax                      POSIX 1003.1-2001 (pax) format\n","    posix                    same as pax\n","    ustar                    POSIX 1003.1-1988 (ustar) format\n","    v7                       old V7 tar format\n","\n","      --old-archive, --portability\n","                             same as --format=v7\n","      --pax-option=keyword[[:]=value][,keyword[[:]=value]]...\n","                             control pax keywords\n","      --posix                same as --format=posix\n","  -V, --label=TEXT           create archive with volume name TEXT; at\n","                             list/extract time, use TEXT as a globbing pattern\n","                             for volume name\n","\n"," Compression options:\n","\n","  -a, --auto-compress        use archive suffix to determine the compression\n","                             program\n","  -I, --use-compress-program=PROG\n","                             filter through PROG (must accept -d)\n","  -j, --bzip2                filter the archive through bzip2\n","  -J, --xz                   filter the archive through xz\n","      --lzip                 filter the archive through lzip\n","      --lzma                 filter the archive through xz\n","      --lzop                 filter the archive through xz\n","      --no-auto-compress     do not use archive suffix to determine the\n","                             compression program\n","  -z, --gzip, --gunzip, --ungzip   filter the archive through gzip\n","  -Z, --compress, --uncompress   filter the archive through compress\n","\n"," Local file selection:\n","\n","      --backup[=CONTROL]     backup before removal, choose version CONTROL\n","  -h, --dereference          follow symlinks; archive and dump the files they\n","                             point to\n","      --hard-dereference     follow hard links; archive and dump the files they\n","                             refer to\n","  -K, --starting-file=MEMBER-NAME\n","                             begin at member MEMBER-NAME when reading the\n","                             archive\n","      --newer-mtime=DATE     compare date and time when data changed only\n","  -N, --newer=DATE-OR-FILE, --after-date=DATE-OR-FILE\n","                             only store files newer than DATE-OR-FILE\n","      --one-file-system      stay in local file system when creating archive\n","  -P, --absolute-names       don't strip leading '/'s from file names\n","      --suffix=STRING        backup before removal, override usual suffix ('~'\n","                             unless overridden by environment variable\n","                             SIMPLE_BACKUP_SUFFIX)\n","\n"," File name transformations:\n","\n","      --strip-components=NUMBER   strip NUMBER leading components from file\n","                             names on extraction\n","      --transform=EXPRESSION, --xform=EXPRESSION\n","                             use sed replace EXPRESSION to transform file\n","                             names\n","\n"," Informative output:\n","\n","      --checkpoint[=NUMBER]  display progress messages every NUMBERth record\n","                             (default 10)\n","      --checkpoint-action=ACTION   execute ACTION on each checkpoint\n","      --full-time            print file time to its full resolution\n","      --index-file=FILE      send verbose output to FILE\n","  -l, --check-links          print a message if not all links are dumped\n","      --no-quote-chars=STRING   disable quoting for characters from STRING\n","      --quote-chars=STRING   additionally quote characters from STRING\n","      --quoting-style=STYLE  set name quoting style; see below for valid STYLE\n","                             values\n","  -R, --block-number         show block number within archive with each message\n","                            \n","      --show-defaults        show tar defaults\n","      --show-omitted-dirs    when listing or extracting, list each directory\n","                             that does not match search criteria\n","      --show-snapshot-field-ranges\n","                             show valid ranges for snapshot-file fields\n","      --show-transformed-names, --show-stored-names\n","                             show file or archive names after transformation\n","      --totals[=SIGNAL]      print total bytes after processing the archive;\n","                             with an argument - print total bytes when this\n","                             SIGNAL is delivered; Allowed signals are: SIGHUP,\n","                             SIGQUIT, SIGINT, SIGUSR1 and SIGUSR2; the names\n","                             without SIG prefix are also accepted\n","      --utc                  print file modification times in UTC\n","  -v, --verbose              verbosely list files processed\n","      --warning=KEYWORD      warning control\n","  -w, --interactive, --confirmation\n","                             ask for confirmation for every action\n","\n"," Compatibility options:\n","\n","  -o                         when creating, same as --old-archive; when\n","                             extracting, same as --no-same-owner\n","\n"," Other options:\n","\n","  -?, --help                 give this help list\n","      --restrict             disable use of some potentially harmful options\n","      --usage                give a short usage message\n","      --version              print program version\n","\n","Mandatory or optional arguments to long options are also mandatory or optional\n","for any corresponding short options.\n","\n","The backup suffix is '~', unless set with --suffix or SIMPLE_BACKUP_SUFFIX.\n","The version control may be set with --backup or VERSION_CONTROL, values are:\n","\n","  none, off       never make backups\n","  t, numbered     make numbered backups\n","  nil, existing   numbered if numbered backups exist, simple otherwise\n","  never, simple   always make simple backups\n","\n","Valid arguments for the --quoting-style option are:\n","\n","  literal\n","  shell\n","  shell-always\n","  c\n","  c-maybe\n","  escape\n","  locale\n","  clocale\n","\n","*This* tar defaults to:\n","--format=gnu -f- -b20 --quoting-style=escape --rmt-command=/usr/lib/tar/rmt\n","--rsh-command=/usr/bin/rsh\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NwqstZ22bfsM","colab_type":"code","colab":{}},"source":["from pytorch_transformers.modeling_bert import BertPreTrainedModel, BertModel\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sHcXuYC4kAzI","colab_type":"code","outputId":"427dc29e-ea94-4cd1-b71d-936a2c17592c","executionInfo":{"status":"ok","timestamp":1579249385976,"user_tz":-330,"elapsed":1090,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad), sum(p.numel() for p in model.parameters()) \n","\n","print(f'The model has {count_parameters(model)} trainable parameters')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["The model has (9052110, 9052110) trainable parameters\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XldyNFfElWg5","colab_type":"code","colab":{}},"source":["from torch import nn\n","class GRUNet(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, embedding_dim,num_emd, drop_prob=0.2):\n","        super(GRUNet, self).__init__()\n","        self.hidden_dim = hidden_dim\n","        self.n_layers = n_layers\n","        \n","        self.embedding = nn.Embedding(embedding_dim=embedding_dim, num_embeddings=num_emd, padding_idx=0)\n","        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","        self.softmax = nn.Softmax(dim=2)\n","        \n","    def forward(self, x,length):\n","        out = self.embedding(x)\n","#         print(f'before sum {out.shape}')\n","#         print(out, length)\n","        out = out.sum(dim=-2)\n","#         print(f'before packing {out.shape}')\n","        out = torch.nn.utils.rnn.pack_padded_sequence(out, lengths=length, batch_first=True, enforce_sorted=True)\n","#         print(f'after packing {out.data.shape}')\n","           \n","        packed_output, h = self.gru(out)\n","        out, out_lengths = torch.nn.utils.rnn.pad_packed_sequence(packed_output,batch_first=True)\n","#         print(f'after unpacking {out.shape} {out_lengths.shape}')\n","        out = self.fc(out)\n","\n","        out=self.softmax(out)\n","        return out, h\n","    \n","    def init_hidden(self, batch_size):\n","        weight = next(self.parameters()).data\n","        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n","        return hidden"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W_NxTSHdlZf2","colab_type":"code","colab":{}},"source":["model = GRUNet(input_dim=10, hidden_dim=1000, output_dim=10, n_layers=2, embedding_dim=10, num_emd=10 )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wXsO6QUVlqJp","colab_type":"code","colab":{}},"source":["import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XcZLk7BTLHZ5","colab_type":"code","colab":{}},"source":["%pycat pybert/io/bert_processor.py"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UeZG6pLioIdg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"26a9afe7-ecc1-4b0f-c9bb-a711a8ae129c","executionInfo":{"status":"ok","timestamp":1579507551861,"user_tz":-330,"elapsed":3169,"user":{"displayName":"Sai Teja","photoUrl":"","userId":"03261428593993315926"}}},"source":["%%writefile pybert/io/bert_processor.py\n","import csv\n","import torch\n","import numpy as np\n","from ..common.tools import load_pickle\n","from ..common.tools import logger\n","from ..callback.progressbar import ProgressBar\n","from torch.utils.data import TensorDataset\n","from pytorch_transformers import BertTokenizer\n","\n","class InputExample(object):\n","    def __init__(self, guid, text_a, text_b=None, label=None):\n","        \"\"\"Constructs a InputExample.\n","        Args:\n","            guid: Unique id for the example.\n","            text_a: string. The untokenized text of the first sequence. For single\n","            sequence tasks, only this sequence must be specified.\n","            text_b: (Optional) string. The untokenized text of the second sequence.\n","            Only must be specified for sequence pair tasks.\n","            label: (Optional) string. The label of the example. This should be\n","            specified for train and dev examples, but not for test examples.\n","        \"\"\"\n","        self.guid   = guid\n","        self.text_a = text_a\n","        self.text_b = text_b\n","        self.label  = label\n","\n","class InputFeature(object):\n","    '''\n","    A single set of features of data.\n","    '''\n","    def __init__(self,input_ids,input_mask,segment_ids,label_id,input_len):\n","        self.input_ids   = input_ids\n","        self.input_mask  = input_mask\n","        self.segment_ids = segment_ids\n","        self.label_id    = label_id\n","        self.input_len = input_len\n","\n","class BertProcessor(object):\n","    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n","\n","    def __init__(self,vocab_path,do_lower_case):\n","        self.tokenizer = BertTokenizer(vocab_path,do_lower_case)\n","\n","    def get_train(self, data_file):\n","        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n","        return self.read_data(data_file)\n","\n","    def get_dev(self, data_file):\n","        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n","        return self.read_data(data_file)\n","\n","    def get_test(self,lines):\n","        return lines\n","\n","    def get_labels(self):\n","        \"\"\"Gets the list of labels for this data set.\"\"\"\n","        return [\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\n","\n","    @classmethod\n","    def read_data(cls, input_file,quotechar = None):\n","        \"\"\"Reads a tab separated value file.\"\"\"\n","        if 'pkl' in str(input_file):\n","            lines = load_pickle(input_file)\n","        else:\n","            lines = input_file\n","        return lines\n","\n","    def truncate_seq_pair(self,tokens_a,tokens_b,max_length):\n","        # This is a simple heuristic which will always truncate the longer sequence\n","        # one token at a time. This makes more sense than truncating an equal percent\n","        # of tokens from each, since if one sequence is very short then each token\n","        # that's truncated likely contains more information than a longer sequence.\n","        while True:\n","            total_length = len(tokens_a) + len(tokens_b)\n","            if total_length <= max_length:\n","                break\n","            if len(tokens_a) > len(tokens_b):\n","                tokens_a.pop()\n","            else:\n","                tokens_b.pop()\n","\n","    def create_examples(self,lines,example_type,cached_examples_file):\n","        '''\n","        Creates examples for data\n","        '''\n","        pbar = ProgressBar(n_total = len(lines))\n","        if cached_examples_file.exists():\n","            logger.info(\"Loading examples from cached file %s\", cached_examples_file)\n","            examples = torch.load(cached_examples_file)\n","        else:\n","            examples = []\n","            for i,line in enumerate(lines):\n","                guid = '%s-%d'%(example_type,i)\n","                text_a = line[0]\n","                label = line[1]\n","                # print('label',label)\n","                if isinstance(label,str):\n","                    label = [np.float(x) for x in label.split(\",\")]\n","                else:\n","                    label = [np.float(x) for x in label]\n","                text_b = None\n","                example = InputExample(guid = guid,text_a = text_a,text_b=text_b,label= label)\n","                examples.append(example)\n","                pbar.batch_step(step=i,info={},bar_type='create examples')\n","            logger.info(\"Saving examples into cached file %s\", cached_examples_file)\n","            torch.save(examples, cached_examples_file)\n","        return examples\n","\n","    def create_features(self,examples,max_seq_len,cached_features_file):\n","        '''\n","        # The convention in BERT is:\n","        # (a) For sequence pairs:\n","        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n","        #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n","        # (b) For single sequences:\n","        #  tokens:   [CLS] the dog is hairy . [SEP]\n","        #  type_ids:   0   0   0   0  0     0   0\n","        '''\n","        pbar = ProgressBar(n_total=len(examples))\n","        if cached_features_file.exists():\n","            logger.info(\"Loading features from cached file %s\", cached_features_file)\n","            features = torch.load(cached_features_file)\n","        else:\n","            features = []\n","            for ex_id,example in enumerate(examples):\n","                tokens_a = self.tokenizer.tokenize(example.text_a)\n","                tokens_b = None\n","                label_id = example.label\n","\n","                if example.text_b:\n","                    tokens_b = self.tokenizer.tokenize(example.text_b)\n","                    # Modifies `tokens_a` and `tokens_b` in place so that the total\n","                    # length is less than the specified length.\n","                    # Account for [CLS], [SEP], [SEP] with \"- 3\"\n","                    self.truncate_seq_pair(tokens_a,tokens_b,max_length = max_seq_len - 3)\n","                else:\n","                    # Account for [CLS] and [SEP] with '-2'\n","                    if len(tokens_a) > max_seq_len - 2:\n","                        tokens_a = tokens_a[:max_seq_len - 2]\n","                tokens = ['[CLS]'] + tokens_a + ['[SEP]']\n","                segment_ids = [0] * len(tokens)\n","                if tokens_b:\n","                    tokens += tokens_b + ['[SEP]']\n","                    segment_ids += [1] * (len(tokens_b) + 1)\n","\n","                input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n","                input_mask = [1] * len(input_ids)\n","                padding = [0] * (max_seq_len - len(input_ids))\n","                input_len = len(input_ids)\n","\n","                input_ids   += padding\n","                input_mask  += padding\n","                segment_ids += padding\n","\n","                assert len(input_ids) == max_seq_len\n","                assert len(input_mask) == max_seq_len\n","                assert len(segment_ids) == max_seq_len\n","\n","                if ex_id < 1000:\n","                    logger.info(\"*** Example ***\")\n","                    logger.info(f\"guid: {example.guid}\" % ())\n","                    logger.info(f\"tokens: {' '.join([str(x) for x in tokens])}\")\n","                    logger.info(f\"input_ids: {' '.join([str(x) for x in input_ids])}\")\n","                    logger.info(f\"input_mask: {' '.join([str(x) for x in input_mask])}\")\n","                    logger.info(f\"segment_ids: {' '.join([str(x) for x in segment_ids])}\")\n","\n","                feature = InputFeature(input_ids = input_ids,\n","                                       input_mask = input_mask,\n","                                       segment_ids = segment_ids,\n","                                       label_id = label_id,\n","                                       input_len = input_len)\n","                features.append(feature)\n","                pbar.batch_step(step=ex_id, info={}, bar_type='create features')\n","            logger.info(\"Saving features into cached file %s\", cached_features_file)\n","            torch.save(features, cached_features_file)\n","        return features\n","\n","    def create_dataset(self,features,is_sorted = False):\n","        # Convert to Tensors and build dataset\n","        if is_sorted:\n","            logger.info(\"sorted data by th length of input\")\n","            features = sorted(features,key=lambda x:x.input_len,reverse=True)\n","        all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n","        all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n","        all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n","#        for i in features:\n","#            print(i.label_id)\n","        all_label_ids = torch.tensor([f.label_id for f in features],dtype=torch.long)\n","        dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n","        return dataset\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Overwriting pybert/io/bert_processor.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6PF5jdVioVWp","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}